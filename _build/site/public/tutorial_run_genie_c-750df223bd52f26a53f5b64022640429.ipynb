{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4babcf9d-ed6b-4fcf-afe6-b1017e8821e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install pyg_lib torch_scatter torch_sparse torch_cluster torch_spline_conv -f https://data.pyg.org/whl/torch-2.6.0+cpu.html\n",
    "import os\n",
    "os.chdir('/home/jovyan/shared/shortcourses/crescent_ml_2025/Ferndale2/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4af6ad8-5aa1-4779-9ccd-8dc608cad551",
   "metadata": {},
   "source": [
    "# Initialize\n",
    "<pre>\n",
    "We import the dependencies, load the trained GNN and travel time neural network, and the initial picks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a931c026-01e6-4b15-88fe-eb9a131c59c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "import glob\n",
    "from obspy.geodetics.base import calc_vincenty_inverse\n",
    "\n",
    "## Make this file self-contained.\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import h5py\n",
    "import os\n",
    "import obspy\n",
    "from obspy.core import UTCDateTime\n",
    "from obspy.clients.fdsn.client import Client\n",
    "from sklearn.metrics import pairwise_distances as pd\n",
    "from scipy.signal import fftconvolve\n",
    "from scipy.spatial import cKDTree\n",
    "import time\n",
    "from torch_cluster import knn\n",
    "from torch_geometric.utils import remove_self_loops, subgraph\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from scipy.spatial import ConvexHull\n",
    "from scipy.spatial import Delaunay\n",
    "from torch_geometric.data import Data\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from matplotlib.colors import Normalize\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.utils import softmax\n",
    "from sklearn.cluster import SpectralClustering\n",
    "from torch.autograd import Variable\n",
    "from numpy.matlib import repmat\n",
    "from scipy.stats import chi2\n",
    "import pathlib\n",
    "import itertools\n",
    "import sys\n",
    "\n",
    "from scipy.signal import find_peaks\n",
    "from torch_geometric.utils import to_networkx, to_undirected, from_networkx\n",
    "from obspy.geodetics.base import calc_vincenty_inverse\n",
    "import matplotlib.gridspec as gridspec\n",
    "import networkx as nx\n",
    "import cvxpy as cp\n",
    "import glob\n",
    "\n",
    "from utils import *\n",
    "from module import *\n",
    "from process_utils import *\n",
    "\n",
    "## This code can be run on cuda, though\n",
    "## in general, it often makes sense to run this script on seperate\n",
    "## jobs/cpus for many days simulataneously (using argv[1]; \n",
    "## e.g., call \"python process_continuous_days.py n\" for many different n\n",
    "## integers and each instance will run day t0_init + n\n",
    "## sbatch or a bash script can call this file for a independent set of cpu threads\n",
    "## (each for a different n, or, day).\n",
    "\n",
    "path_to_file = str(pathlib.Path().absolute())\n",
    "seperator = '\\\\' if '\\\\' in path_to_file else '/'\n",
    "path_to_file += seperator\n",
    "\n",
    "## Note, parameters d_deg (multiple re-uses), n_batch, n_segment, min_picks, \n",
    "## and max_sources (per connected graph), max_splits can be moved to config\n",
    "## Also replace \"iz1, iz2 = np.where(Out_2 > 0.0025)\" with specified thresh\n",
    "\n",
    "## Need to update how extract_inputs_from_data_fixed_grids_with_phase_type uses a variable t_win parammeter, \n",
    "## and also adding inputs of training_params, graph_params, pred_params\n",
    "\n",
    "# The first system argument (after the file name; e.g., argvs[1]) is an integer used to select which\n",
    "# day in the %s_process_days_list_ver_%d.txt file each call of this script will compute\n",
    "# argvs = sys.argv\n",
    "# if len(argvs) < 2: \n",
    "# \targvs.append(0) \n",
    "\n",
    "# if len(argvs) < 3:\n",
    "# \targvs.append(0)\n",
    "\n",
    "argvs = [0,0,0]\n",
    "\n",
    "# This index can also be incremented by the larger value: argvs[2]*offset_increment (defined in process_config)\n",
    "# to help process very large pick lists with a combinations of using job arrays\n",
    "# to increment argvs[1], and seperate sbatch scripts incrementing argvs[2]\n",
    "\n",
    "day_select = int(argvs[1])\n",
    "offset_select = int(argvs[2])\n",
    "\n",
    "print('name of program is %s'%argvs[0])\n",
    "print('day is %s'%argvs[1])\n",
    "\n",
    "### Settings: ###\n",
    "\n",
    "with open('process_config.yaml', 'r') as file:\n",
    "    process_config = yaml.safe_load(file)\n",
    "\n",
    "## Load Processing settings\n",
    "n_ver_load = process_config['n_ver_load']\n",
    "n_step_load = process_config['n_step_load']\n",
    "n_save_ver = process_config['n_save_ver']\n",
    "n_ver_picks = process_config['n_ver_picks']\n",
    "\n",
    "template_ver = process_config['template_ver']\n",
    "vel_model_ver = process_config['vel_model_ver']\n",
    "process_days_ver = process_config['process_days_ver']\n",
    "\n",
    "offset_increment = process_config['offset_increment']\n",
    "n_rand_query = process_config['n_rand_query']\n",
    "n_query_grid = process_config['n_query_grid']\n",
    "\n",
    "thresh = process_config['thresh'] # Threshold to declare detection\n",
    "thresh_assoc = process_config['thresh_assoc'] # Threshold to declare src-arrival association\n",
    "\n",
    "## removed\n",
    "# spr = process_config['spr'] # Sampling rate to save temporal predictions\n",
    "# tc_win = process_config['tc_win'] # Temporal window (s) to link events in Local Marching\n",
    "# sp_win = process_config['sp_win'] # Distance (m) to link events in Local Marching\n",
    "\n",
    "break_win = process_config['break_win'] # Temporal window to find disjoint groups of sources, \n",
    "## so can run Local Marching without memory issues.\n",
    "spr_picks = process_config['spr_picks'] # Assumed sampling rate of picks \n",
    "## (can be 1 if absolute times are us5d for pick time values)\n",
    "\n",
    "## Removed\n",
    "# d_win = process_config['d_win'] ## Lat and lon window to re-locate initial source detetections with refined sampling over\n",
    "# d_win_depth = process_config['d_win_depth'] ## Depth window to re-locate initial source detetections with refined sampling over\n",
    "\n",
    "dx_depth = 50.0 ## This is not longer used (unless particle swarm location is used) # process_config['dx_depth'] ## Depth resolution to locate events with travel time based re-location\n",
    "\n",
    "## removed\n",
    "# step = process_config['step']\n",
    "# step_abs = process_config['step_abs']\n",
    "\n",
    "use_quality_check = process_config['use_quality_check'] ## If True, check all associated picks and set a maximum allowed relative error after obtaining initial location\n",
    "max_relative_error = process_config['max_relative_error'] ## 0.15 corresponds to 15% maximum relative error allowed\n",
    "min_time_buffer = process_config['min_time_buffer'] ## Uses this time (seconds) as a minimum residual time, beneath which, the relative error criterion is ignored (i.e., an associated pick is removed if both the relative error > max_relative_error and the residual > min_time_buffer)\n",
    "\n",
    "cost_value = process_config['cost_value'] # If use expanded competitve assignment, then this is the fixed cost applied per source\n",
    "## when optimizing joint source-arrival assignments between nearby sources. The value is in terms of the \n",
    "## `sum' over the predicted source-arrival assignment for each pick. Ideally could make this number more\n",
    "## adpative, potentially with number of stations or number of possible observing picks for each event. \n",
    "\n",
    "device = torch.device(process_config['device']) ## Right now, this isn't updated to work with cuda, since\n",
    "if (process_config['device'] == 'cuda')*(torch.cuda.is_available() == False):\n",
    "\tprint('No cuda available, using cpu')\n",
    "\tdevice = torch.device('cpu')\n",
    "\n",
    "## the necessary variables do not have .to(device) at the right places\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "compute_magnitudes = process_config['compute_magnitudes']\n",
    "min_log_amplitude_val = process_config['min_log_amplitude_val']\n",
    "use_topography = process_config['use_topography']\n",
    "process_known_events = process_config['process_known_events']\n",
    "load_prebuilt_sampling_grid = process_config['load_prebuilt_sampling_grid']\n",
    "use_expanded_competitive_assignment = process_config['use_expanded_competitive_assignment']\n",
    "use_differential_evolution_location = process_config['use_differential_evolution_location']\n",
    "\n",
    "## Minimum required picks and stations per event\n",
    "min_required_picks = process_config['min_required_picks']\n",
    "min_required_sta = process_config['min_required_sta']\n",
    "\n",
    "### Begin automated processing ###\n",
    "print('Beginning processing')\n",
    "\n",
    "# Load configuration from YAML\n",
    "with open('config.yaml', 'r') as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "k_sta_edges = config['k_sta_edges']\n",
    "k_spc_edges = config['k_spc_edges']\n",
    "k_time_edges = config['k_time_edges']\n",
    "\n",
    "name_of_project = config['name_of_project']\n",
    "use_physics_informed = config['use_physics_informed']\n",
    "use_phase_types = config['use_phase_types']\n",
    "use_subgraph = config['use_subgraph']\n",
    "if use_subgraph == True:\n",
    "    max_deg_offset = config['max_deg_offset']\n",
    "    k_nearest_pairs = config['k_nearest_pairs']\n",
    "\t\n",
    "# Load day to process\n",
    "z = open(path_to_file + '%s_process_days_list_ver_%d.txt'%(name_of_project, process_days_ver), 'r')\n",
    "lines = z.readlines()\n",
    "z.close()\n",
    "day_select_val = day_select + offset_select*offset_increment\n",
    "if '/' in lines[day_select_val]:\n",
    "\tdate = lines[day_select_val].split('/')\n",
    "elif ',' in lines[day_select_val]:\n",
    "\tdate = lines[day_select_val].split(',')\n",
    "else:\n",
    "\tdate = lines[day_select_val].split(' ')\t\n",
    "date = np.array([int(date[0]), int(date[1]), int(date[2])])\n",
    "\n",
    "# Load region\n",
    "z = np.load(path_to_file + '%s_region.npz'%name_of_project)\n",
    "lat_range, lon_range, depth_range, deg_pad = z['lat_range'], z['lon_range'], z['depth_range'], z['deg_pad']\n",
    "z.close()\n",
    "\n",
    "# Load templates\n",
    "z = np.load(path_to_file + 'Grids/%s_seismic_network_templates_ver_%d.npz'%(name_of_project, template_ver))\n",
    "x_grids = z['x_grids']\n",
    "z.close()\n",
    "\n",
    "# Load stations\n",
    "z = np.load(path_to_file + '%s_stations.npz'%name_of_project)\n",
    "locs, stas, mn, rbest = z['locs'], z['stas'], z['mn'], z['rbest']\n",
    "z.close()\n",
    "\n",
    "## Create path to write files\n",
    "write_training_file = path_to_file + 'GNN_TrainedModels/' + name_of_project + '_'\n",
    "\n",
    "z = np.load(write_training_file + 'trained_gnn_model_step_%d_ver_%d_losses.npz'%(n_step_load, n_ver_load))\n",
    "training_params = z['training_params']\n",
    "graph_params = z['graph_params']\n",
    "pred_params = z['pred_params']\n",
    "z.close()\n",
    "\n",
    "lat_range_extend = [lat_range[0] - deg_pad, lat_range[1] + deg_pad]\n",
    "lon_range_extend = [lon_range[0] - deg_pad, lon_range[1] + deg_pad]\n",
    "\n",
    "scale_x = np.array([lat_range[1] - lat_range[0], lon_range[1] - lon_range[0], depth_range[1] - depth_range[0]]).reshape(1,-1)\n",
    "offset_x = np.array([lat_range[0], lon_range[0], depth_range[0]]).reshape(1,-1)\n",
    "scale_x_extend = np.array([lat_range_extend[1] - lat_range_extend[0], lon_range_extend[1] - lon_range_extend[0], depth_range[1] - depth_range[0]]).reshape(1,-1)\n",
    "offset_x_extend = np.array([lat_range_extend[0], lon_range_extend[0], depth_range[0]]).reshape(1,-1)\n",
    "\n",
    "\n",
    "rbest_cuda = torch.Tensor(rbest).to(device)\n",
    "mn_cuda = torch.Tensor(mn).to(device)\n",
    "\n",
    "\n",
    "# use_spherical = False\n",
    "if config['use_spherical'] == True:\n",
    "\n",
    "\tearth_radius = 6371e3\n",
    "\tftrns1 = lambda x: (rbest @ (lla2ecef(x, e = 0.0, a = earth_radius) - mn).T).T # just subtract mean\n",
    "\tftrns2 = lambda x: ecef2lla((rbest.T @ x.T).T + mn, e = 0.0, a = earth_radius) # just subtract mean\n",
    "\n",
    "\tftrns1_diff = lambda x: (rbest_cuda @ (lla2ecef_diff(x, e = 0.0, a = earth_radius, device = device) - mn_cuda).T).T # just subtract mean\n",
    "\tftrns2_diff = lambda x: ecef2lla_diff((rbest_cuda.T @ x.T).T + mn_cuda, e = 0.0, a = earth_radius, device = device) # just subtract mean\n",
    "\n",
    "else:\n",
    "\n",
    "\tearth_radius = 6378137.0\n",
    "\tftrns1 = lambda x: (rbest @ (lla2ecef(x) - mn).T).T # just subtract mean\n",
    "\tftrns2 = lambda x: ecef2lla((rbest.T @ x.T).T + mn) # just subtract mean\n",
    "\n",
    "\tftrns1_diff = lambda x: (rbest_cuda @ (lla2ecef_diff(x, device = device) - mn_cuda).T).T # just subtract mean\n",
    "\tftrns2_diff = lambda x: ecef2lla_diff((rbest_cuda.T @ x.T).T + mn_cuda, device = device) # just subtract mean\n",
    "\n",
    "if config['train_travel_time_neural_network'] == False:\n",
    "\n",
    "\t## Load travel times\n",
    "\tz = np.load(path_to_file + '1D_Velocity_Models_Regional/%s_1d_velocity_model_ver_%d.npz'%(name_of_project, vel_model_ver))\n",
    "\t\n",
    "\tTp = z['Tp_interp']\n",
    "\tTs = z['Ts_interp']\n",
    "\t\n",
    "\tlocs_ref = z['locs_ref']\n",
    "\tX = z['X']\n",
    "\tz.close()\n",
    "\t\n",
    "\tx1 = np.unique(X[:,0])\n",
    "\tx2 = np.unique(X[:,1])\n",
    "\tx3 = np.unique(X[:,2])\n",
    "\tassert(len(x1)*len(x2)*len(x3) == X.shape[0])\n",
    "\t\n",
    "\t\n",
    "\t## Load fixed grid for velocity models\n",
    "\tXmin = X.min(0)\n",
    "\tDx = [np.diff(x1[0:2]),np.diff(x2[0:2]),np.diff(x3[0:2])]\n",
    "\tMn = np.array([len(x3), len(x1)*len(x3), 1]) ## Is this off by one index? E.g., np.where(np.diff(xx[:,0]) != 0)[0] isn't exactly len(x3)\n",
    "\tN = np.array([len(x1), len(x2), len(x3)])\n",
    "\tX0 = np.array([locs_ref[0,0], locs_ref[0,1], 0.0]).reshape(1,-1)\n",
    "\t\n",
    "\ttrv = interp_1D_velocity_model_to_3D_travel_times(X, locs_ref, Xmin, X0, Dx, Mn, Tp, Ts, N, ftrns1, ftrns2, device = device) # .to(device)\n",
    "\n",
    "elif config['train_travel_time_neural_network'] == True:\n",
    "\n",
    "\tn_ver_trv_time_model_load = vel_model_ver\n",
    "\ttrv = load_travel_time_neural_network(path_to_file, ftrns1_diff, ftrns2_diff, n_ver_trv_time_model_load, use_physics_informed = use_physics_informed, device = device)\n",
    "\ttrv_pairwise = load_travel_time_neural_network(path_to_file, ftrns1_diff, ftrns2_diff, n_ver_trv_time_model_load, method = 'direct', use_physics_informed = use_physics_informed, device = device)\n",
    "\ttrv_pairwise1 = load_travel_time_neural_network(path_to_file, ftrns1_diff, ftrns2_diff, n_ver_trv_time_model_load, method = 'direct', return_model = True, use_physics_informed = use_physics_informed, device = device)\n",
    "\n",
    "if (use_differential_evolution_location == False)*(config['train_travel_time_neural_network'] == False):\n",
    "\thull = ConvexHull(X)\n",
    "\thull = hull.points[hull.vertices]\n",
    "else:\n",
    "\thull = []\n",
    "\n",
    "print('Finished initial imports')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c111d6f0-1de3-446a-b572-02bb3dfe8fe9",
   "metadata": {},
   "source": [
    "# Check Basic Network Files\n",
    "<pre>\n",
    "(i). Let's check the station numpy array (\"locs\")\n",
    "    <pre>\n",
    "    \"locs\" is a numpy array of columns of: (lat, lon, depth)\n",
    "    \n",
    "<pre>\n",
    "<pre>\n",
    "\n",
    "(ii). And then let's check the travel time neural network calculator (\"trv\")\n",
    "    <pre>\n",
    "    \"trv\" is a function that maps trv(sta_locations, src_locations) -- > travel time predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8e81d4-9704-4133-ad87-5559c1fa4998",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot stations\n",
    "fig, ax = plt.subplots(figsize = [8,8])\n",
    "plt.scatter(locs[:,1], locs[:,0], c = 'r', marker = '^')\n",
    "ax.set_aspect(1.0/np.cos(np.pi*locs[:,0].mean()/180.0)) ## Set projection aspect ratio\n",
    "d_pad = 0.5 ## Pad 0.5 degrees to plot axes\n",
    "ax.set_xlim(np.array(ax.get_xlim()) + np.array([-d_pad, d_pad]))\n",
    "ax.set_ylim(np.array(ax.get_ylim()) + np.array([-d_pad, d_pad]))\n",
    "plt.show(block = False)\n",
    "print('Available stations:')\n",
    "print(stas) ## Station names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83677cde-080f-413a-b87b-a993e4127f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's also download the USGS catalog from the day being processed and add these to the map (12/20/22)\n",
    "t0 = UTCDateTime(date[0], date[1], date[2])\n",
    "min_magnitude = 0.1\n",
    "srcs_known = download_catalog(lat_range, lon_range, min_magnitude, t0, t0 + 3600*24, t0 = t0, client = 'USGS')[0] # Choose client\n",
    "print('Downloaded %d known events'%len(srcs_known))\n",
    "\n",
    "## Plot stations\n",
    "fig, ax = plt.subplots(figsize = [8,8])\n",
    "plt.scatter(locs[:,1], locs[:,0], c = 'r', marker = '^')\n",
    "plt.scatter(srcs_known[:,1], srcs_known[:,0], c = 'C0', s = 6.5, marker = 'o', label = 'USGS')\n",
    "ax.set_aspect(1.0/np.cos(np.pi*locs[:,0].mean()/180.0)) ## Set projection aspect ratio\n",
    "d_pad = 0.5 ## Pad 0.5 degrees to plot axes\n",
    "ax.set_xlim(np.array(ax.get_xlim()) + np.array([-d_pad, d_pad]))\n",
    "ax.set_ylim(np.array(ax.get_ylim()) + np.array([-d_pad, d_pad]))\n",
    "plt.legend()\n",
    "plt.show(block = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b95b390-cbdc-4894-83b8-0b008b78c699",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's also look at USGS event magnitudes vs. time\n",
    "fig, ax = plt.subplots(figsize = [8,7])\n",
    "plt.scatter(srcs_known[:,3]/3600.0, srcs_known[:,4], label = 'USGS') ## Times (s) vs. magnitudes\n",
    "plt.xlabel('Time (hrs) since %d/%d/%d'%(date[0], date[1], date[2]))\n",
    "plt.ylabel('Magnitude')\n",
    "plt.text(61000/3600.0, 5.7, 'Where is the mainshock?')\n",
    "plt.text(61000/3600.0, 5.45, 'Where are the aftershocks?')\n",
    "plt.legend()\n",
    "plt.show(block = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a52d69f-d436-444f-a949-5567ae45d28c",
   "metadata": {},
   "source": [
    "# Now do remaining imports and initilizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0473005f-f9e7-48df-aeb2-e250a702dc51",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check if knn is working on cuda\n",
    "if device.type == 'cuda' or device.type == 'cpu':\n",
    "\tcheck_len = knn(torch.rand(10,3).to(device), torch.rand(10,3).to(device), k = 5).numel()\n",
    "\tif check_len != 100: # If it's less than 2 * 10 * 5, there's an issue\n",
    "\t\traise SystemError('Issue with knn on cuda for some versions of pytorch geometric and cuda')\n",
    "\n",
    "\tcheck_len = knn(10.0*torch.rand(200,3).to(device), 10.0*torch.rand(100,3).to(device), k = 15).numel()\n",
    "\tif check_len != 3000: # If it's less than 2 * 10 * 5, there's an issue\n",
    "\t\traise SystemError('Issue with knn on cuda for some versions of pytorch geometric and cuda')\n",
    "\n",
    "x_grids, x_grids_edges, x_grids_trv, x_grids_trv_pointers_p, x_grids_trv_pointers_s, x_grids_trv_refs, max_t = load_templates_region(trv, locs, x_grids, ftrns1, training_params, graph_params, pred_params, device = device)\n",
    "x_grids_cart_torch = [torch.Tensor(ftrns1(x_grids[i])).to(device) for i in range(len(x_grids))]\n",
    "\n",
    "# mz = GCN_Detection_Network_extended(ftrns1_diff, ftrns2_diff)\n",
    "\n",
    "load_model = True\n",
    "if load_model == True:\n",
    "\n",
    "\tmz_list = []\n",
    "\tfor i in range(len(x_grids)):\n",
    "\t\tmz_slice = GCN_Detection_Network_extended(ftrns1_diff, ftrns2_diff, device = device).to(device)\n",
    "\t\tmz_slice.load_state_dict(torch.load(path_to_file + 'GNN_TrainedModels/%s_trained_gnn_model_step_%d_ver_%d.h5'%(name_of_project, n_step_load, n_ver_load), map_location = device))\n",
    "\t\tmz_slice.eval()\n",
    "\t\tmz_list.append(mz_slice)\n",
    "\n",
    "failed = []\n",
    "plot_on = False\n",
    "\n",
    "\n",
    "\n",
    "day_len = 3600*24\n",
    "# t_win = config['t_win']\n",
    "\n",
    "use_adaptive_window = True\n",
    "if use_adaptive_window == True:\n",
    "\tn_resolution = 9 ## The discretization of the source time function output\n",
    "\tt_win = np.round(np.copy(np.array([2*pred_params[2]]))[0], 2) ## Set window size to the source kernel width (i.e., prediction window is of length +/- src_t_kernel, or [-src_t_kernel + t0, t0 + src_t_kernel])\n",
    "\tdt_win = np.diff(np.linspace(-t_win/2.0, t_win/2.0, n_resolution))[0]\n",
    "\tassert(t_win == pred_params[0])\n",
    "else:\n",
    "\tdt_win = 1.0 ## Default version\n",
    "\tt_win = 10.0\n",
    "\n",
    "# step_size = process_config['step_size'] # 'full'\n",
    "step_size = process_config['step_size']\n",
    "if step_size == 'full':\n",
    "\tstep = n_resolution*dt_win\n",
    "\tn_overlap = 1.0\n",
    "elif step_size == 'partial':\n",
    "\tstep = (n_resolution/3)*dt_win\n",
    "\tn_overlap = 3.0 ## Check this\n",
    "\tassert(use_adaptive_window == True)\n",
    "\tassert(n_resolution == 9) ## hard coded for length nine vector (must check which time fractions of total window stack uniformly over time when doing sliding window and stacking)\n",
    "elif step_size == 'half':\n",
    "\tstep = int(np.floor((n_resolution/2)))*dt_win\n",
    "\tn_overlap = 2.0 ## Check this\n",
    "\tassert(use_adaptive_window == True)\n",
    "\tassert(n_resolution == 9) ## hard coded for length nine vector (must check which time fractions of total window stack uniformly over time when doing sliding window and stacking)\n",
    "\n",
    "# pred_params = [t_win, kernel_sig_t, src_t_kernel, src_x_kernel, src_depth_kernel]\n",
    "tc_win = pred_params[2]*1.25 # process_config['tc_win'] # Temporal window (s) to link events in Local Marching\n",
    "sp_win = pred_params[3]*1.25 # process_config['sp_win'] # Distance (m) to link events in Local Marching\n",
    "d_win = pred_params[3]*1.25/110e3 ## Converting km to degrees, roughly\n",
    "d_win_depth = pred_params[4]*1.25  ## proportional to depth kernel\n",
    "src_t_kernel = pred_params[2] ## temporal source kernel size\n",
    "\n",
    "## Make topography surface\n",
    "if (use_topography == True)*(os.path.isfile(path_to_file + 'Grids' + seperator + '%s_surface_elevation.npz'%name_of_project) == True):\n",
    "\tsurface_profile = np.load(path_to_file + 'Grids' + seperator + '%s_surface_elevation.npz'%name_of_project)['surface_profile']\n",
    "elif use_topography == True: ## If no surface profile saved, then interpolate a regular grid based on saved station elevations\n",
    "\tn_surface = 100 ## Default resolution of surface\n",
    "\tx1_surface, x2_surface = np.linspace(lat_range_extend[0], lat_range_extend[1], n_surface), np.linspace(lon_range_extend[0], lon_range_extend[1], n_surface)\n",
    "\tx11_surface, x12_surface = np.meshgrid(x1_surface, x2_surface)\n",
    "\tsurface_profile = np.concatenate((x11_surface.reshape(-1,1), x12_surface.reshape(-1,1), np.zeros((len(x11_surface.reshape(-1)),1))), axis = 1)\n",
    "\ttree_sta = cKDTree(ftrns1(locs))\n",
    "\tsurface_profile[:,2] = locs[tree_sta.query(ftrns1(surface_profile))[1],2]\n",
    "\t## Average the profile\n",
    "\tedges_surface = knn(torch.Tensor(ftrns1(surface_profile)), torch.Tensor(ftrns1(surface_profile)), k = 15).flip(0).contiguous()\n",
    "\tsurface_profile[:,2] = scatter(torch.Tensor(surface_profile[edges_surface[0].cpu().detach().numpy(),2].reshape(-1,1)), edges_surface[1], dim = 0, reduce = 'mean').cpu().detach().numpy().reshape(-1)\n",
    "else:\n",
    "\tsurface_profile = None\n",
    "\n",
    "# d_win = process_config['d_win'] ## Lat and lon window to re-locate initial source detetections with refined sampling over\n",
    "# d_win_depth = process_config['d_win_depth'] ## Depth window to re-locate initial source detetections with refined sampling over\n",
    "\n",
    "\n",
    "tsteps = np.arange(0, day_len, step) ## Make step any of 3 options for efficiency... (a full step, a hald step, and a fifth step?)\n",
    "tsteps_abs = np.arange(-t_win/2.0, day_len + t_win/2.0 + dt_win, dt_win) ## Fixed solution grid, assume 1 second\n",
    "tree_tsteps = cKDTree(tsteps_abs.reshape(-1,1))\n",
    "\n",
    "\n",
    "# tsteps_abs_cat = cKDTree(tsteps.reshape(-1,1)) ## Make this tree, so can look up nearest time for all cat.\n",
    "print('\\nDoing adaptive time steps (%0.2f win, %0.2f step, %0.2f overlap), to avoid issue of repeating time samples'%(dt_win, step, n_overlap))\n",
    "\n",
    "\n",
    "n_batch = 1\n",
    "n_batches = int(np.floor(len(tsteps)/n_batch))\n",
    "n_extra = len(tsteps) - n_batches*n_batch\n",
    "# n_overlap = int(t_win/step) # check this\n",
    "\n",
    "\n",
    "# n_samples = int(250e3)\n",
    "plot_on = False\n",
    "save_on = True\n",
    "\n",
    "d_deg = 0.1 ## leads to 42 k grid?\n",
    "print('Going to compute sources only in interior region')\n",
    "\n",
    "x1 = np.arange(lat_range[0], lat_range[1] + d_deg, d_deg)\n",
    "x2 = np.arange(lon_range[0], lon_range[1] + d_deg, d_deg)\n",
    "\n",
    "# load_prebuilt_sampling_grid = True\n",
    "n_ver_sampling_grid = 1\n",
    "if (load_prebuilt_sampling_grid == True)*(os.path.isfile(path_to_file + 'Grids' + seperator + 'prebuilt_sampling_grid_ver_%d.npz'%n_ver_sampling_grid) == True):\n",
    "\t\n",
    "\tz = np.load(path_to_file + 'Grids' + seperator + 'prebuilt_sampling_grid_ver_%d.npz'%n_ver_sampling_grid)\n",
    "\tX_query = z['X_query']\n",
    "\tX_query_cart = torch.Tensor(ftrns1(np.copy(X_query))).to(device)\n",
    "\tz.close()\n",
    "\n",
    "else:\t\n",
    "\n",
    "\tuse_irregular_reference_grid = True ## Could add a different function to create the initial grid sampling points\n",
    "\tif use_irregular_reference_grid == True:\n",
    "\t\tX_query = kmeans_packing_sampling_points(scale_x, offset_x, 3, n_query_grid, ftrns1, n_batch = 3000, n_steps = 3000, n_sim = 1)[0]\n",
    "\t\tX_query_cart = torch.Tensor(ftrns1(np.copy(X_query))).to(device)\n",
    "\telse:\n",
    "\t\tx3 = np.arange(-45e3, 5e3 + 10e3, 20e3)\n",
    "\t\tx11, x12, x13 = np.meshgrid(x1, x2, x3)\n",
    "\t\txx = np.concatenate((x11.reshape(-1,1), x12.reshape(-1,1), x13.reshape(-1,1)), axis = 1)\n",
    "\t\tX_query = np.copy(xx)\n",
    "\t\tX_query_cart = torch.Tensor(ftrns1(np.copy(xx))).to(device)\n",
    "\n",
    "\tif load_prebuilt_sampling_grid == True:\n",
    "\t\tnp.savez_compressed(path_to_file + 'Grids' + seperator + 'prebuilt_sampling_grid_ver_%d.npz'%n_ver_sampling_grid, X_query = X_query)\n",
    "\n",
    "loaded_mag_model = False\n",
    "if compute_magnitudes == True:\n",
    "\n",
    "\ttry:\n",
    "\t\tn_mag_ver = 1\n",
    "\t\tmags_supp = np.load(path_to_file + 'trained_magnitude_model_ver_%d_supplemental.npz'%n_mag_ver)\n",
    "\t\tmag_grid, k_grid = mags_supp['mag_grid'], int(mags_supp['k_grid'])\n",
    "\t\tmags = Magnitude(torch.Tensor(locs).to(device), torch.Tensor(mag_grid).to(device), ftrns1_diff, ftrns2_diff, k = k_grid, device = device)\n",
    "\t\tmags.load_state_dict(torch.load(path_to_file + 'trained_magnitude_model_ver_%d.hdf5'%n_mag_ver, map_location = device))\n",
    "\t\tloaded_mag_model = True\n",
    "\t\tprint('Will compute magnitudes since a magnitude model was loaded')\n",
    "\t\n",
    "\texcept:\n",
    "\t\tprint('Will not compute magnitudes since no magnitude model was loaded')\n",
    "\t\tloaded_mag_model = False\n",
    "\n",
    "else:\n",
    "\tprint('Will not compute magnitudes since compute_magnitudes = False')\t\n",
    "\n",
    "# Window over which to \"relocate\" each \n",
    "# event with denser sampling from GNN output\n",
    "d_deg = 0.018 ## Is this discretization being preserved?\n",
    "x1 = np.linspace(-d_win, d_win, 15)\n",
    "x2 = np.linspace(-d_win, d_win, 15)\n",
    "x3 = np.linspace(-d_win_depth, d_win_depth, 15)\n",
    "x11, x12, x13 = np.meshgrid(x1, x2, x3)\n",
    "xx = np.concatenate((x11.reshape(-1,1), x12.reshape(-1,1), x13.reshape(-1,1)), axis = 1)\n",
    "X_offset = np.copy(xx)\n",
    "\n",
    "check_if_finished = False\n",
    "\n",
    "# print('Should change this to use all grids, potentially')\n",
    "x_grid_ind_list = np.sort(np.random.choice(len(x_grids), size = 1, replace = False)) # 15\n",
    "x_grid_ind_list_1 = np.sort(np.random.choice(len(x_grids), size = len(x_grids), replace = False)) # 15\n",
    "\n",
    "use_only_one_grid = process_config['use_only_one_grid']\n",
    "if use_only_one_grid == True:\n",
    "\t# x_grid_ind_list_1 = np.array([x_grid_ind_list_1[np.random.choice(len(x_grid_ind_list_l))]])\n",
    "\tx_grid_ind_list_1 = np.copy(x_grid_ind_list)\n",
    "\n",
    "assert (max([abs(len(x_grids_trv_refs[0]) - len(x_grids_trv_refs[j])) for j in range(len(x_grids_trv_refs))]) == 0)\n",
    "\n",
    "n_scale_x_grid = len(x_grid_ind_list)\n",
    "n_scale_x_grid_1 = len(x_grid_ind_list_1)\n",
    "\n",
    "fail_count = 0\n",
    "success_count = 0\n",
    "\n",
    "## Extra default parameters\n",
    "n_src_query = 1\n",
    "x_src_query = locs.mean(0).reshape(1,-1) # arbitrary point to query source-arrival associations during initial processing pass\n",
    "x_src_query_cart = torch.Tensor(ftrns1(x_src_query))\n",
    "tq_sample = torch.rand(n_src_query)*t_win - t_win/2.0 # Note this part!\n",
    "tq_sample = torch.zeros(1)\n",
    "tq = torch.arange(-t_win/2.0, t_win/2.0 + dt_win, dt_win).reshape(-1,1).float().to(device)\n",
    "\n",
    "yr, mo, dy = date[0], date[1], date[2]\n",
    "date = np.array([yr, mo, dy])\n",
    "\n",
    "P, ind_use = load_picks(path_to_file, date, spr_picks = spr_picks, n_ver = n_ver_picks)\n",
    "# P, ind_use = load_picks(path_to_file, date, locs, stas, lat_range, lon_range, spr_picks = spr_picks, n_ver = n_ver_picks)\n",
    "print('\\nUsing %d total picks (%d P and %d S waves) \\n'%(len(P), len(np.where(P[:,4] == 0)[0]), len(np.where(P[:,4] == 1)[0])))\n",
    "\n",
    "if use_phase_types == False:\n",
    "\tP[:,4] = 0 ## No phase types\n",
    "locs_use = locs[ind_use]\n",
    "arrivals_tree = cKDTree(P[:,0][:,None])\n",
    "\n",
    "use_updated_input = True\n",
    "dt_embed_discretize = np.round(pred_params[1]/10.0, 2) # 0.05 ## Picks are discretized to this amount if using updated input to speed up input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b71bdf-974f-42d9-bd4b-579f6f4c2f03",
   "metadata": {},
   "source": [
    "# Now let's visualize the picks\n",
    "<pre>\n",
    "The picks are stored in \"P\" variable; each row is (time, station index, amplitude, prob pick, phase type)\n",
    "\n",
    "Station indices are sorted by increasing latitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133c4b57-15db-48c8-8b95-c710df0aca13",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's plot the picks and zoom in on a few intervals\n",
    "fig, ax = plt.subplots(figsize = [10,6])\n",
    "t_window = [3600.0, 3600.0 + 60.0*30] ## Plot a 30 min window\n",
    "i1 = np.where(((P[:,0] > t_window[0])*(P[:,0] < t_window[1]))*(P[:,4] == 0))[0]\n",
    "i2 = np.where(((P[:,0] > t_window[0])*(P[:,0] < t_window[1]))*(P[:,4] == 1))[0]\n",
    "ax.scatter(P[i1,0], P[i1,1], c = 'C0')\n",
    "ax.scatter(P[i2,0], P[i2,1], c = 'C1')\n",
    "ax.set_xlabel('Time (s)')\n",
    "ax.set_ylabel('Station Index')\n",
    "plt.show(block = False)\n",
    "print('\\nIs this before or after the mainshock? Why do you think so?\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159bb8c6-56e4-4264-9fda-4b1408d74cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Here's another interval\n",
    "fig, ax = plt.subplots(figsize = [10,6]) # 10:34:25\n",
    "t_window = [3600.0*12, 3600.0*12 + 60.0*15] ## Plot a 15 min window\n",
    "i1 = np.where(((P[:,0] > t_window[0])*(P[:,0] < t_window[1]))*(P[:,4] == 0))[0]\n",
    "i2 = np.where(((P[:,0] > t_window[0])*(P[:,0] < t_window[1]))*(P[:,4] == 1))[0]\n",
    "ax.scatter(P[i1,0], P[i1,1], c = 'C0')\n",
    "ax.scatter(P[i2,0], P[i2,1], c = 'C1')\n",
    "ax.set_xlabel('Time (s)')\n",
    "ax.set_ylabel('Station Index')\n",
    "plt.show(block = False)\n",
    "print('\\nIs this before or after the mainshock? Why do you think so?\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc683912-f126-45a9-820f-ebd06d2d7a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "## And one more interval\n",
    "fig, ax = plt.subplots(figsize = [10,6])\n",
    "t_window = [3600.0*10.5, 3600.0*10.5 + 60.0*15] ## Plot a 15 min window\n",
    "i1 = np.where(((P[:,0] > t_window[0])*(P[:,0] < t_window[1]))*(P[:,4] == 0))[0]\n",
    "i2 = np.where(((P[:,0] > t_window[0])*(P[:,0] < t_window[1]))*(P[:,4] == 1))[0]\n",
    "ax.scatter(P[i1,0], P[i1,1], c = 'C0')\n",
    "ax.scatter(P[i2,0], P[i2,1], c = 'C1')\n",
    "ax.set_xlabel('Time (s)')\n",
    "ax.set_ylabel('Station Index')\n",
    "plt.show(block = False)\n",
    "print('\\nIs this before or after the mainshock? Why do you think so?\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437ab0dc-89a0-4ef2-a412-f9325dee690f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Here is the same interval, where picks are colored by log(amplitude)\n",
    "fig, ax = plt.subplots(figsize = [10,6])\n",
    "t_window = [3600.0*10.5, 3600.0*10.5 + 60.0*15] ## Plot a 15 min window\n",
    "i1 = np.where(((P[:,0] > t_window[0])*(P[:,0] < t_window[1]))*(P[:,4] == 0))[0]\n",
    "i2 = np.where(((P[:,0] > t_window[0])*(P[:,0] < t_window[1]))*(P[:,4] == 1))[0]\n",
    "ax.scatter(P[i1,0], P[i1,1], c = np.log10(P[i1,2]), label = r'$\\log_{10}$(amp)')\n",
    "ax.scatter(P[i2,0], P[i2,1], c = np.log10(P[i2,2]))\n",
    "ax.set_xlabel('Time (s)')\n",
    "ax.set_xlabel('Station Index')\n",
    "plt.legend()\n",
    "plt.show(block = False)\n",
    "print('\\nWhy does the pattern of colors look like this?\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0fbc61-9860-43e1-b591-8b43d7bdfbe1",
   "metadata": {},
   "source": [
    "# Let's check how reliable our travel time calculator and picks are\n",
    "<pre>\n",
    "To do this we will predict travel times across network for a few fixed source locations and compare against the observed picks.\n",
    "\n",
    "<pre>\n",
    "    (i). First we'll do random source coordinates (and origin times)\n",
    "    \n",
    "<pre>\n",
    "    (ii). Then we'll do \"known\" USGS source coordinates (and origin times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d759ea-997f-4a35-a41f-d2845e44a689",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Predict travel times (random source coordinatea and origin times)\n",
    "n_srcs = 10 ## Number sources\n",
    "t_win_rand = 10 ## Window over which sources occur (minutes)\n",
    "src_random = np.random.rand(n_srcs,3)*scale_x + offset_x # (note: scale_x and offset_x define region of interest)\n",
    "src_origin = np.random.rand(n_srcs)*60.0*t_win_rand + 12*3600.0 ## Nucleate sources within 10 minute interval, starting after 12:00:00 UTC.\n",
    "\n",
    "print('Random sources')\n",
    "print(src_random[:,0:2])\n",
    "## This calls the travel time nueral network for chosen stations and sources\n",
    "trv_out = trv(torch.Tensor(locs).to(device), torch.Tensor(src_random).to(device)).cpu().detach().numpy() + src_origin.reshape(-1,1,1)\n",
    "print('Shape of trv_out is (num_sources, num_stations, num_phase_type)')\n",
    "\n",
    "## Plot picks with travel times (from random source coordinates) overlaying the picks\n",
    "fig, ax = plt.subplots(figsize = [10,6]) # 10:34:25\n",
    "t_window = [3600.0*12, 3600.0*12 + 60.0*t_win_rand] ## Plot a 10 min window\n",
    "i1 = np.where(((P[:,0] > t_window[0])*(P[:,0] < t_window[1]))*(P[:,4] == 0))[0]\n",
    "i2 = np.where(((P[:,0] > t_window[0])*(P[:,0] < t_window[1]))*(P[:,4] == 1))[0]\n",
    "ax.scatter(P[i1,0], P[i1,1], c = 'C0')\n",
    "ax.scatter(P[i2,0], P[i2,1], c = 'C1')\n",
    "ax.plot(trv_out[:,:,0].T, np.arange(trv_out.shape[1]), c = 'b')\n",
    "ax.plot(trv_out[:,:,1].T, np.arange(trv_out.shape[1]), c = 'r')\n",
    "ax.set_xlabel('Time (s)')\n",
    "ax.set_xlabel('Station Index')\n",
    "plt.show(block = False)\n",
    "print('\\nWhy do moveout curves look like this? Are they meaningful?\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30ad558-f327-4be4-9bf4-020cf38b9fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Predict travel times (known source coordinates)\n",
    "\n",
    "# Find known sources with origin time within pre-specific window\n",
    "ifind = np.where((srcs_known[:,3] < t_window[1])*(srcs_known[:,3] > t_window[0]))[0]\n",
    "\n",
    "## This calls the travel time nueral network for chosen stations and sources\n",
    "trv_out = trv(torch.Tensor(locs).to(device), torch.Tensor(srcs_known[ifind]).to(device)).cpu().detach().numpy() + srcs_known[ifind,3].reshape(-1,1,1)\n",
    "print('Shape of trv_out is (num_sources, num_stations, num_phase_type)')\n",
    "\n",
    "## Plot picks with travel times (from random source coordinates) overlaying the picks\n",
    "fig, ax = plt.subplots(figsize = [10,6]) # 10:34:25\n",
    "# t_window = [3600.0*12, 3600.0*12 + 60.0*t_win_rand] ## Plot a 10 min window\n",
    "i1 = np.where(((P[:,0] > t_window[0])*(P[:,0] < t_window[1]))*(P[:,4] == 0))[0]\n",
    "i2 = np.where(((P[:,0] > t_window[0])*(P[:,0] < t_window[1]))*(P[:,4] == 1))[0]\n",
    "ax.scatter(P[i1,0], P[i1,1], c = 'C0')\n",
    "ax.scatter(P[i2,0], P[i2,1], c = 'C1')\n",
    "ax.plot(trv_out[:,:,0].T, np.arange(trv_out.shape[1]), c = 'b')\n",
    "ax.plot(trv_out[:,:,1].T, np.arange(trv_out.shape[1]), c = 'r')\n",
    "ax.set_xlabel('Time (s)')\n",
    "ax.set_ylabel('Station Index')\n",
    "plt.show(block = False)\n",
    "\n",
    "## Inset of the previous plot\n",
    "fig, ax = plt.subplots(figsize = [10,6]) # 10:34:25\n",
    "# t_window = [3600.0*12, 3600.0*12 + 60.0*t_win_rand] ## Plot a 10 min window\n",
    "i1 = np.where(((P[:,0] > t_window[0])*(P[:,0] < t_window[1]))*(P[:,4] == 0))[0]\n",
    "i2 = np.where(((P[:,0] > t_window[0])*(P[:,0] < t_window[1]))*(P[:,4] == 1))[0]\n",
    "ax.scatter(P[i1,0], P[i1,1], c = 'C0')\n",
    "ax.scatter(P[i2,0], P[i2,1], c = 'C1')\n",
    "ax.plot(trv_out[:,:,0].T, np.arange(trv_out.shape[1]), c = 'b')\n",
    "ax.plot(trv_out[:,:,1].T, np.arange(trv_out.shape[1]), c = 'r')\n",
    "ax.set_xlabel('Time (s)')\n",
    "ax.set_ylabel('Station Index')\n",
    "ax.set_xlim([43400, 43650])\n",
    "plt.show(block = False)\n",
    "\n",
    "print('\\nWhy do moveout curves look like this? Are they meaningful?\\n')\n",
    "\n",
    "print('\\nCan the result shown be improved? How?')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f10d13-6d76-45c3-9315-4c210df0bd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now we'll show the previous plot sorted by distance from Ferndale epicenter\n",
    "\n",
    "isort = np.argsort(np.linalg.norm(np.array([40.525, -124.423]).reshape(1,-1) - locs[:,0:2], axis = 1))\n",
    "perm_vec = (-1*np.ones(len(locs))).astype('int')\n",
    "perm_vec[isort] = np.arange(len(isort))\n",
    "assert(perm_vec.min() > -1)\n",
    "\n",
    "fig, ax = plt.subplots(figsize = [10,6]) # 10:34:25\n",
    "# t_window = [3600.0*12, 3600.0*12 + 60.0*t_win_rand] ## Plot a 10 min window\n",
    "i1 = np.where(((P[:,0] > t_window[0])*(P[:,0] < t_window[1]))*(P[:,4] == 0))[0]\n",
    "i2 = np.where(((P[:,0] > t_window[0])*(P[:,0] < t_window[1]))*(P[:,4] == 1))[0]\n",
    "ax.scatter(P[i1,0], perm_vec[P[i1,1].astype('int')], c = 'C0')\n",
    "ax.scatter(P[i2,0], perm_vec[P[i2,1].astype('int')], c = 'C1')\n",
    "ax.plot(trv_out[:,isort,0].T, np.arange(trv_out.shape[1]), c = 'b')\n",
    "ax.plot(trv_out[:,isort,1].T, np.arange(trv_out.shape[1]), c = 'r')\n",
    "ax.set_xlabel('Time (s)')\n",
    "ax.set_xlabel('Station Index (sorted from Ferndale epicenter)')\n",
    "ax.set_xlim([43400, 43650])\n",
    "plt.show(block = False)\n",
    "\n",
    "print('\\nWhich sources are close to Ferndale? Which ones arnt?\\n')\n",
    "print('\\nWhich event is the biggest? Why?')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87166426-13aa-4e5f-b809-4dad243d20d9",
   "metadata": {},
   "source": [
    "# Is the travel time calculator working? Do the picks seem plausible? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7e6f52-df91-416f-abd0-9f71f9561143",
   "metadata": {},
   "source": [
    "# Lastly let's check the station and source graphs we're using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2f764c-5acf-44f9-b04b-560e735b8582",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize = [18,8], sharex = True, sharey = True)\n",
    "\n",
    "## We'll use K-nearest-neighbor graphs.\n",
    "\n",
    "## We also use our (lat,long,depth) --> (x,y,z) projection function, \"ftrns1\", to fasciliate distance measurements\n",
    "\n",
    "## Build station graph\n",
    "edges_sta = knn(torch.Tensor(ftrns1(locs)).to(device)/1000.0, torch.Tensor(ftrns1(locs)).to(device)/1000.0, k = k_sta_edges).flip(0).contiguous().cpu().detach().numpy()\n",
    "# edges_sta = knn(torch.Tensor(locs[:,0:2]).to(device), torch.Tensor(locs[:,0:2]).to(device), k = 2) # .flip(0).contiguous().cpu().detach().numpy()\n",
    "\n",
    "## Build source graph\n",
    "edges_src = knn(torch.Tensor(ftrns1(x_grids[0])).to(device)/1000.0, torch.Tensor(ftrns1(x_grids[0])).to(device)/1000.0, k = k_spc_edges).flip(0).contiguous().cpu().detach().numpy()\n",
    "\n",
    "ax[0].scatter(locs[:,1], locs[:,0], c = 'r', marker = '^', label = 'Stations')\n",
    "for j in range(edges_sta.shape[1]):\n",
    "    ax[0].plot(np.concatenate((locs[edges_sta[0][j],1].reshape(1,-1), locs[edges_sta[1][j],1].reshape(1,-1)), axis = 1).T, np.concatenate((locs[edges_sta[0][j],0].reshape(1,-1), locs[edges_sta[1][j],0].reshape(1,-1)), axis = 1).T, c = 'black', alpha = 0.5)\n",
    "ax[1].scatter(x_grids[0][:,1], x_grids[0][:,0], c = 'r', label = 'Source nodes', marker = 'o')\n",
    "for j in range(edges_src.shape[1]):\n",
    "    ax[1].plot(np.concatenate((x_grids[0][edges_src[0][j],1].reshape(1,-1), x_grids[0][edges_src[1][j],1].reshape(1,-1)), axis = 1).T, np.concatenate((x_grids[0][edges_src[0][j],0].reshape(1,-1), x_grids[0][edges_src[1][j],0].reshape(1,-1)), axis = 1).T, c = 'black', alpha = 0.5)\n",
    "d_pad = 0.5 ## Pad 0.5 degrees to plot axes\n",
    "ax[1].scatter(srcs_known[:,1], srcs_known[:,0], c = 'm', s = 8.0, marker = 'o', label = 'USGS')\n",
    "ax[0].set_xlim(np.array(ax[0].get_xlim()) + np.array([-d_pad, d_pad]))\n",
    "ax[0].set_ylim(np.array(ax[0].get_ylim()) + np.array([-d_pad, d_pad]))\n",
    "ax[1].set_xlim(np.array(ax[0].get_xlim())) # + np.array([-d_pad, d_pad]))\n",
    "ax[1].set_ylim(np.array(ax[0].get_ylim())) # + np.array([-d_pad, d_pad]))\n",
    "# ax[0].set_title('Stations')\n",
    "# ax[0].set_title('Sources')\n",
    "ax[0].legend()\n",
    "ax[1].legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dacc269-f6d1-42f6-918a-ba2fd6417836",
   "metadata": {},
   "source": [
    "# Now let's run the associator. We'll plot a few intermediate outputs and results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4f4f0e-76a7-4b44-9c57-da1726b021e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## To speed things up for this test, we're only going to process 3 hours of data (around the mainshock)\n",
    "clip_win_start = 37000.0\n",
    "clip_win_duration = 3600.0*3\n",
    "ifind = np.where((P[:,0] > clip_win_start)*(P[:,0] < (clip_win_start + clip_win_duration)))[0]\n",
    "n_total = len(P)\n",
    "P = P[ifind] ## Only keeping a subset of the picks\n",
    "print('Keeping %d of %d total picks (%d P waves and %d S waves)'%(len(P), n_total, len(np.where(P[:,4] == 0)[0]), len(np.where(P[:,4] == 1)[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789f0849-ee7f-4da7-b3e7-58dcc79b9855",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Now we're going to run GENIE over all the continuous space-time data.\n",
    "\n",
    "# The first step detects all space-time maximum outputs (e.g., candidate sources), \n",
    "# by passing in the inputs over sliced windows to the GNN, storing outputs, \n",
    "# stepping forward in time, and stacking overlapping portions.\n",
    "\n",
    "# Then it runs a \"peak finding\" algorithm to detect a discrete number of candidate sources in time,\n",
    "# and then the \"local marching\" method to merge nearby maxima in space.\n",
    "\n",
    "trv_out_src = trv(torch.Tensor(locs[ind_use]).to(device), torch.Tensor(x_src_query).to(device)).detach() # .to(device)\n",
    "locs_use_cart_torch = torch.Tensor(ftrns1(locs_use)).to(device)\n",
    "A_src_in_sta_l = []\n",
    "\n",
    "for i in range(len(x_grids)):\n",
    "\n",
    "    if use_subgraph == False:\n",
    "    \n",
    "        # x_grids, x_grids_edges, x_grids_trv, x_grids_trv_pointers_p, x_grids_trv_pointers_s, x_grids_trv_refs\n",
    "        A_sta_sta, A_src_src, A_prod_sta_sta, A_prod_src_src, A_src_in_prod, A_edges_time_p, A_edges_time_s, A_edges_ref = extract_inputs_adjacencies(trv, locs, ind_use, x_grids[i], x_grids_trv[i], x_grids_trv_refs[i], x_grids_trv_pointers_p[i], x_grids_trv_pointers_s[i], ftrns1, graph_params, device = device)\n",
    "\n",
    "        A_src_in_sta = torch.Tensor(np.concatenate((np.tile(np.arange(len(ind_use)), len(x_grids[i])).reshape(1,-1), np.arange(len(x_grids[i])).repeat(len(ind_use), axis = 0).reshape(1,-1)), axis = 0)).long().to(device)\n",
    "        spatial_vals = torch.Tensor(((np.repeat(np.expand_dims(x_grids[i], axis = 1), len(ind_use), axis = 1) - np.repeat(np.expand_dims(locs[ind_use], axis = 0), x_grids[i].shape[0], axis = 0)).reshape(-1,3))/scale_x_extend).to(device)\n",
    "        A_src_in_edges = Data(x = spatial_vals, edge_index = A_src_in_prod).to(device)\n",
    "        A_Lg_in_src = Data(x = spatial_vals, edge_index = torch.Tensor(np.ascontiguousarray(np.flip(A_src_in_prod.cpu().detach().numpy(), axis = 0))).long()).to(device)\n",
    "        trv_out = trv(torch.Tensor(locs[ind_use]).to(device), torch.Tensor(x_grids[i]).to(device)).detach().reshape(-1,2) ## Can replace trv_out with Trv_out\n",
    "        mz_list[i].set_adjacencies(A_prod_sta_sta, A_prod_src_src, A_src_in_edges, A_Lg_in_src, A_src_in_sta, A_src_src, torch.Tensor(A_edges_time_p).long().to(device), torch.Tensor(A_edges_time_s).long().to(device), torch.Tensor(A_edges_ref).to(device), trv_out, torch.Tensor(ftrns1(locs_use)).to(device), torch.Tensor(ftrns1(x_grids[i])).to(device))\n",
    "        A_src_in_sta_l.append(A_src_in_sta.cpu().detach().numpy())\n",
    "\n",
    "    else:\n",
    "\n",
    "        # x_grids, x_grids_edges, x_grids_trv, x_grids_trv_pointers_p, x_grids_trv_pointers_s, x_grids_trv_refs\n",
    "        A_sta_sta, A_src_src, A_prod_sta_sta, A_prod_src_src, A_src_in_prod, A_src_in_sta = extract_inputs_adjacencies_subgraph(locs_use, x_grids[i], ftrns1, ftrns2, max_deg_offset = max_deg_offset, k_nearest_pairs = k_nearest_pairs, k_sta_edges = k_sta_edges, k_spc_edges = k_spc_edges, device = device)\n",
    "        A_edges_time_p, A_edges_time_s, dt_partition = compute_time_embedding_vectors(trv_pairwise, locs_use, x_grids[i], A_src_in_sta, max_t, t_win = t_win, device = device)\n",
    "        spatial_vals = torch.Tensor((x_grids[i][A_src_in_prod[1].cpu().detach().numpy()] - locs_use[A_src_in_sta[0][A_src_in_prod[0]].cpu().detach().numpy()])/scale_x_extend).to(device)\n",
    "        A_src_in_prod = Data(x = spatial_vals, edge_index = A_src_in_prod)\n",
    "        \n",
    "        flipped_edge = torch.Tensor(np.ascontiguousarray(np.flip(A_src_in_prod.edge_index.cpu().detach().numpy(), axis = 0))).long().to(device)\n",
    "        A_src_in_prod_flipped = Data(x = spatial_vals, edge_index = flipped_edge).to(device)\n",
    "        trv_out = trv_pairwise(torch.Tensor(locs_use[A_src_in_sta[0].cpu().detach().numpy()]).to(device), torch.Tensor(x_grids[i][A_src_in_sta[1].cpu().detach().numpy()]).to(device))\n",
    "        mz_list[i].set_adjacencies(A_prod_sta_sta, A_prod_src_src, A_src_in_prod, A_src_in_prod_flipped, A_src_in_sta, A_src_src, torch.Tensor(A_edges_time_p).long().to(device), torch.Tensor(A_edges_time_s).long().to(device), torch.Tensor(dt_partition).to(device), trv_out, torch.Tensor(ftrns1(locs_use)).to(device), torch.Tensor(ftrns1(x_grids[i])).to(device))\n",
    "        A_src_in_sta_l.append(A_src_in_sta.cpu().detach().numpy())\n",
    "\n",
    "check_overflow = True\n",
    "if (use_updated_input == True)*(check_overflow == True): ## Check if embedding correctly preserved all travel time indices (overflow can happen on GPU for very large spatial domains x number of stations when using scatter)\n",
    "    ## Note, must also add check that overflow doesn't happen during the second scatter operation in extract_input_from_data\n",
    "    n_random_check = 5\n",
    "    for i in range(n_random_check): ## n_random_check\n",
    "        ## Simulate picks\n",
    "        src, src_origin = x_grids[0][np.random.choice(len(x_grids[0]))].reshape(1,-1), np.random.rand()*(np.nanmax(P[:,0]) - np.nanmin(P[:,0])) + np.nanmin(P[:,0])\n",
    "        trv_out = trv(torch.Tensor(locs).to(device), torch.Tensor(src).to(device)).cpu().detach().numpy() + src_origin\n",
    "        ikeep = np.sort(np.random.choice(len(ind_use), size = int(np.ceil(len(ind_use)*0.7)), replace = False))\n",
    "        ikeep1 = np.sort(np.random.choice(len(ind_use), size = int(np.ceil(len(ind_use)*0.7)), replace = False))\n",
    "        \n",
    "        P1 = np.concatenate((trv_out[0,ind_use[ikeep],0].reshape(-1,1), ind_use[ikeep].reshape(-1,1), np.zeros((len(ikeep),3))), axis = 1)\n",
    "        P1 = np.concatenate((P1, np.concatenate((trv_out[0,ind_use[ikeep1],1].reshape(-1,1), ind_use[ikeep1].reshape(-1,1), np.zeros((len(ikeep1),2)), np.ones((len(ikeep1),1))), axis = 1)), axis = 0)\n",
    "        # if use_phase_types == False:\n",
    "        # \tP1[:,4] = 0 ## No phase types\n",
    "\n",
    "        x_grid_ind = x_grid_ind_list[0] ## Note: if this fails, essentially dt_embed_discretize is too small (resulting in too many time steps x number stations (combined with max moveout, max_t) leading to too large of graphs in the scatter operation for extracting inputs (e.g., ~ 100 million nodes))\n",
    "        embed_p, embed_s, ind_unique_, abs_time_ref_, n_time_series_, n_sta_unique_ = extract_input_from_data(trv_pairwise, P1, np.array([src_origin]), ind_use, locs, x_grids[x_grid_ind], A_src_in_sta_l[x_grid_ind], trv_times = x_grids_trv[x_grid_ind], max_t = max_t, kernel_sig_t = pred_params[1], dt = dt_embed_discretize, return_embedding = True, device = device)\n",
    "\n",
    "        ## Check positive points\n",
    "        vec_p_ = embed_p.reshape(n_sta_unique_, n_time_series_)\n",
    "        vec_s_ = embed_s.reshape(n_sta_unique_, n_time_series_)\n",
    "        tree_ = cKDTree(ind_unique_.reshape(-1,1))\n",
    "        ip_ = tree_.query(P1[:,1].reshape(-1,1))[1] ## Matched index to unique indices\n",
    "        ip1_, ip2_ = np.where(P1[:,4] == 0)[0], np.where(P1[:,4] == 1)[0]\n",
    "        t_p_, t_s_ = ((P1[ip1_,0] - abs_time_ref_[0])/dt_embed_discretize).astype('int'), ((P1[ip2_,0] - abs_time_ref_[0])/dt_embed_discretize).astype('int')\n",
    "        itp_, its_ = np.where((t_p_ >= 0)*(t_p_ < n_time_series_))[0], np.where((t_s_ >= 0)*(t_s_ < n_time_series_))[0]\n",
    "        val_p_, val_s_ = vec_p_[ip_[ip1_[itp_]], t_p_[itp_]].cpu().detach().numpy(), vec_s_[ip_[ip2_[its_]], t_s_[its_]].cpu().detach().numpy()\n",
    "        if len(val_p_) > 0: assert(val_p_.min() > 0.9)\n",
    "        if len(val_s_) > 0: assert(val_s_.min() > 0.9)\n",
    "        print('Min check val is %0.4f'%np.min(np.concatenate((val_p_, val_s_), axis = 0)))\n",
    "\n",
    "        ## Check zero points\n",
    "        iselect_ = np.sort(np.random.choice(len(P1), size = 10000))\n",
    "        iwhere_p_, iwhere_s_ = np.where(P1[iselect_,4] == 0)[0], np.where(P1[iselect_,4] == 1)[0]\n",
    "        t_rand_p_ = P1[iselect_[iwhere_p_],0] + 4.0*pred_params[1]*np.random.choice([-1.0, 1.0], size = len(iwhere_p_))\n",
    "        t_rand_s_ = P1[iselect_[iwhere_s_],0] + 4.0*pred_params[1]*np.random.choice([-1.0, 1.0], size = len(iwhere_s_))\n",
    "\n",
    "        ip_1_ = tree_.query(P1[iselect_[iwhere_p_],1].reshape(-1,1))[1] ## Matched index to unique indices\n",
    "        ip_2_ = tree_.query(P1[iselect_[iwhere_s_],1].reshape(-1,1))[1] ## Matched index to unique indices\n",
    "        ip1_, ip2_ = np.where(P1[iselect_[iwhere_p_],4] == 0)[0], np.where(P1[iselect_[iwhere_s_],4] == 1)[0]\n",
    "        t_p_, t_s_ = ((t_rand_p_[ip1_] - abs_time_ref_[0])/dt_embed_discretize).astype('int'), ((t_rand_s_[ip2_] - abs_time_ref_[0])/dt_embed_discretize).astype('int')\n",
    "        itp_, its_ = np.where((t_p_ >= 0)*(t_p_ < n_time_series_))[0], np.where((t_s_ >= 0)*(t_s_ < n_time_series_))[0]\n",
    "        val_p_, val_s_ = vec_p_[ip_1_[ip1_[itp_]], t_p_[itp_]].cpu().detach().numpy(), vec_s_[ip_2_[ip2_[its_]], t_s_[its_]].cpu().detach().numpy()\n",
    "        if len(val_p_) > 0: assert(val_p_.max() < 0.1)\n",
    "        if len(val_s_) > 0: assert(val_s_.max() < 0.1)\n",
    "        print('Max check val is %0.4f \\n'%np.max(np.concatenate((val_p_, val_s_), axis = 0)))\n",
    "\n",
    "\n",
    "tree_picks = cKDTree(P[:,0:2]) # based on absolute indices\n",
    "\n",
    "\n",
    "P_perm = np.copy(P)\n",
    "perm_vec = -1*np.ones(locs.shape[0])\n",
    "perm_vec[ind_use] = np.arange(len(ind_use))\n",
    "P_perm[:,1] = perm_vec[P_perm[:,1].astype('int')]\n",
    "\n",
    "if process_known_events == False: # If false, process continuous days\n",
    "    times_need_l = np.copy(tsteps)\n",
    "else:  # If true, process around times of known events\n",
    "    srcs_known_times = np.copy(srcs_known[:,3])\n",
    "    tree_srcs_known_times = cKDTree(tsteps.reshape(-1,1))\n",
    "    ip_nearest_srcs_known_times = tree_srcs_known_times.query(srcs_known_times.reshape(-1,1))[1]\n",
    "    srcs_known_times = tsteps[ip_nearest_srcs_known_times]\n",
    "    times_need_l = np.unique((srcs_known_times.reshape(-1,1) + np.arange(-pred_params[0]*3, pred_params[0]*3 + step, step).reshape(1,-1)).reshape(-1))\n",
    "    \n",
    "## Double check this.\n",
    "n_batches = int(np.floor(len(times_need_l)/n_batch))\n",
    "times_need = [times_need_l[j*n_batch:(j + 1)*n_batch] for j in range(n_batches)]\n",
    "if n_batches*n_batch < len(times_need_l):\n",
    "    times_need.append(times_need_l[n_batches*n_batch::]) ## Add last few samples\n",
    "\n",
    "assert(len(np.hstack(times_need)) == len(np.unique(np.hstack(times_need))))\n",
    "\n",
    "skip_quiescent_intervals = True\n",
    "if skip_quiescent_intervals == True:\n",
    "    min_pick_window = min_required_picks if min_required_picks != False else 1 ## Check windows with at least this many picks\n",
    "    times_ind_need = []\n",
    "    sc_inc = 0\n",
    "    ## Find time window where < min_pick_window occur on the input set, and do not process\n",
    "    for i in range(len(times_need)):\n",
    "        lp = arrivals_tree.query_ball_point(times_need[i].reshape(-1,1) + max_t/2.0, r = t_win + max_t/2.0)\n",
    "        for j in range(len(times_need[i])):\n",
    "            if len(list(lp[j])) >= min_pick_window:\n",
    "                times_ind_need.append(sc_inc)\n",
    "            sc_inc += 1\n",
    "\n",
    "    ## Subselect times_need_l\n",
    "    if len(times_ind_need) > 0:\n",
    "        times_need_l = times_need_l[np.array(times_ind_need)]\n",
    "\n",
    "        ## Double check this.\n",
    "        n_batches = int(np.floor(len(times_need_l)/n_batch))\n",
    "        times_need = [times_need_l[j*n_batch:(j + 1)*n_batch] for j in range(n_batches)]\n",
    "        if n_batches*n_batch < len(times_need_l):\n",
    "            times_need.append(times_need_l[n_batches*n_batch::]) ## Add last few samples\n",
    "\n",
    "        assert(len(np.hstack(times_need)) == len(np.unique(np.hstack(times_need))))\n",
    "        print('Processing %d inputs in %d batches'%(len(times_need_l), len(times_need)))\n",
    "\n",
    "    else:\n",
    "        print('No windows with > %d picks (min_pick_window)'%min_pick_window)\n",
    "        print('Stopping processing')\n",
    "        # continue\n",
    "\n",
    "# Out_1 = np.zeros((x_grids[x_grid_ind_list[0]].shape[0], len(tsteps_abs))) # assumes all grids have same cardinality\n",
    "Out_2 = np.zeros((X_query_cart.shape[0], len(tsteps_abs)))\n",
    "\n",
    "# with torch.no_grad():\n",
    "\n",
    "for n in range(len(times_need)):\n",
    "\n",
    "    tsteps_slice = times_need[n]\n",
    "    tsteps_slice_indices = tree_tsteps.query(tsteps_slice.reshape(-1,1))[1]\n",
    "    # out_cumulative_max = 0.0\n",
    "    \n",
    "    for x_grid_ind in x_grid_ind_list:\n",
    "\n",
    "        if use_updated_input == False:\n",
    "        \n",
    "            ## It might be more efficient if Inpts, Masks, lp_times, and lp_stations were already on Tensor\n",
    "            [Inpts, Masks], [lp_times, lp_stations, lp_phases, lp_meta] = extract_inputs_from_data_fixed_grids_with_phase_type(trv, locs, ind_use, P, P[:,4], arrivals_tree, tsteps_slice, x_grids[x_grid_ind], x_grids_trv[x_grid_ind], lat_range_extend, lon_range_extend, depth_range, max_t, training_params, graph_params, pred_params, ftrns1, ftrns2)\n",
    "\n",
    "        else:\n",
    "\n",
    "            [Inpts, Masks], [lp_times, lp_stations, lp_phases, lp_meta] = extract_input_from_data(trv_pairwise, P, tsteps_slice, ind_use, locs, x_grids[x_grid_ind], A_src_in_sta_l[x_grid_ind], trv_times = x_grids_trv[x_grid_ind], max_t = max_t, kernel_sig_t = pred_params[1], dt = dt_embed_discretize, device = device)\n",
    "\n",
    "        # if use_subgraph == True:\n",
    "        # \tfor i in range(len(Inpts)):\n",
    "        #                 Inpts[i] = Inpts[i].reshape(x_grids[x_grid_ind].shape[0], locs_use.shape[0], 4)[A_src_in_sta_l[x_grid_ind][1], A_src_in_sta_l[x_grid_ind][0]] # .cpu().detach().numpy()\n",
    "        #                 Masks[i] = Masks[i].reshape(x_grids[x_grid_ind].shape[0], locs_use.shape[0], 4)[A_src_in_sta_l[x_grid_ind][1], A_src_in_sta_l[x_grid_ind][0]] # .cpu().detach().numpy()\n",
    "        \n",
    "        if use_phase_types == False:\n",
    "            for i in range(len(Inpts)):\n",
    "                Inpts[i][:,2::] = 0.0 ## Phase type informed features zeroed out\n",
    "                Masks[i][:,2::] = 0.0\n",
    "        \n",
    "        for i0 in range(len(tsteps_slice)):\n",
    "\n",
    "            if len(lp_times[i0]) == 0:\n",
    "                continue ## It will fail if len(lp_times[i0]) == 0!\n",
    "\n",
    "            ## Note: this is repeated, for each pass of the x_grid loop.\n",
    "            ip_need = tree_tsteps.query(tsteps_abs[tsteps_slice_indices[i0]] + np.arange(-t_win/2.0, t_win/2.0 + dt_win, dt_win).reshape(-1,1)) # tq\n",
    "\n",
    "            ## Need x_src_query_cart and trv_out_src\n",
    "            out = mz_list[x_grid_ind].forward_fixed_source(torch.Tensor(Inpts[i0]).to(device), torch.Tensor(Masks[i0]).to(device), torch.Tensor(lp_times[i0]).to(device), torch.Tensor(lp_stations[i0]).long().to(device), torch.Tensor(lp_phases[i0].reshape(-1,1)).float().to(device), torch.Tensor(ftrns1(locs_use)).to(device), x_grids_cart_torch[x_grid_ind], X_query_cart, tq)\n",
    "\n",
    "            # Out_1[:,ip_need[1]] += out[0][:,0:-1,0].cpu().detach().numpy()/n_overlap/n_scale_x_grid\n",
    "            # Out_2[:,ip_need[1]] += out[1][:,0:-1,0].cpu().detach().numpy()/n_overlap/n_scale_x_grid\n",
    "\n",
    "            if step_size == 'half': ## In this case, must drop last index (so all points are stacked over equally; this is hard-coded for an output size of 9 steps)\n",
    "                Out_2[:,ip_need[1][0:-1]] += out[1][:,0:-1,0].cpu().detach().numpy()/n_overlap/n_scale_x_grid\n",
    "            else:\n",
    "                Out_2[:,ip_need[1]] += out[1][:,:,0].cpu().detach().numpy()/n_overlap/n_scale_x_grid\n",
    "        \n",
    "            # out_cumulative_max += out[1].max().item() if (out[1].max().item() > 0.075) else 0\n",
    "            if ((np.mod(i0, 50) == 0) + ((np.mod(i0, 5) == 0)))*(out[1].max().item() > 0.3):\n",
    "                print('%d %d %0.2f'%(n, i0, out[1].max().item()))\n",
    "                # out_cumulative_max = 0.0 # Reset moving detection metric print output\n",
    "\n",
    "iz1, iz2 = np.where(Out_2 > 0.01) # Zeros out all values less than this\n",
    "Out_2_sparse = np.concatenate((iz1.reshape(-1,1), iz2.reshape(-1,1), Out_2[iz1,iz2].reshape(-1,1)), axis = 1)\n",
    "\n",
    "xq = np.copy(X_query)\n",
    "ts = np.copy(tsteps_abs)\n",
    "assert(np.diff(ts)[0] == dt_win)\n",
    "\n",
    "print('Begin peak finding')\n",
    "use_sparse_peak_finding = False\n",
    "if use_sparse_peak_finding == True:\n",
    "\n",
    "    srcs_init = []\n",
    "    for i in range(xq.shape[0]):\n",
    "\n",
    "        ifind_x = np.where(iz1 == i)[0]\n",
    "        if len(ifind_x) > 0:\n",
    "\n",
    "            trace = np.zeros(len(ts))\n",
    "            trace[iz2[ifind_x]] = Out_2_sparse[ifind_x,2]\n",
    "            \n",
    "            # ip = np.where(Out[:,i] > thresh)[0]\n",
    "            ip = find_peaks(trace, height = thresh, distance = int(src_t_kernel/dt_win)) ## Note: should add prominence as thresh/2.0, which might help detect nearby events. Also, why is min time spacing set as 2 seconds?\n",
    "            if len(ip[0]) > 0: # why use xq here?\n",
    "                val = np.concatenate((xq[i,:].reshape(1,-1)*np.ones((len(ip[0]),3)), ts[ip[0]].reshape(-1,1), ip[1]['peak_heights'].reshape(-1,1)), axis = 1)\n",
    "                srcs_init.append(val)\t\t\n",
    "\n",
    "else:\n",
    "\n",
    "    Out = np.zeros((X_query.shape[0], len(tsteps_abs))) ## Use dense out array\n",
    "    Out[Out_2_sparse[:,0].astype('int'), Out_2_sparse[:,1].astype('int')] = Out_2_sparse[:,2]\n",
    "\n",
    "    srcs_init = []\n",
    "    for i in range(Out.shape[0]):\n",
    "        # ip = np.where(Out[:,i] > thresh)[0]\n",
    "        ip = find_peaks(Out[i,:], height = thresh, distance = int(src_t_kernel/dt_win)) ## Note: should add prominence as thresh/2.0, which might help detect nearby events. Also, why is min time spacing set as 2 seconds?\n",
    "        if len(ip[0]) > 0: # why use xq here?\n",
    "            val = np.concatenate((xq[i,:].reshape(1,-1)*np.ones((len(ip[0]),3)), ts[ip[0]].reshape(-1,1), ip[1]['peak_heights'].reshape(-1,1)), axis = 1)\n",
    "            srcs_init.append(val)\n",
    "\n",
    "# if len(srcs_init) == 0:\n",
    "#     continue ## No sources, continue\n",
    "\n",
    "srcs_init = np.vstack(srcs_init) # Could this have memory issues?\n",
    "\n",
    "srcs_init = srcs_init[np.argsort(srcs_init[:,3]),:]\n",
    "tdiff = np.diff(srcs_init[:,3])\n",
    "ibreak = np.where(tdiff >= break_win)[0]\n",
    "srcs_groups_l = []\n",
    "ind_inc = 0\n",
    "\n",
    "if len(ibreak) > 0:\n",
    "    for i in range(len(ibreak)):\n",
    "        srcs_groups_l.append(srcs_init[np.arange(ind_inc, ibreak[i] + 1)])\n",
    "        ind_inc = ibreak[i] + 1\n",
    "    if len(np.vstack(srcs_groups_l)) < srcs_init.shape[0]:\n",
    "        srcs_groups_l.append(srcs_init[(ibreak[-1] + 1)::])\n",
    "else:\n",
    "    srcs_groups_l.append(srcs_init)\n",
    "\n",
    "print('Begin local marching')\n",
    "srcs_l = []\n",
    "scale_depth_clustering = 0.2\n",
    "for i in range(len(srcs_groups_l)):\n",
    "    if len(srcs_groups_l[i]) == 1:\n",
    "        srcs_l.append(srcs_groups_l[i])\n",
    "    else:\n",
    "        mp = LocalMarching(device = device)\n",
    "        srcs_out = mp(srcs_groups_l[i], ftrns1, tc_win = tc_win, sp_win = sp_win, scale_depth = scale_depth_clustering)\n",
    "        if len(srcs_out) > 0:\n",
    "            srcs_l.append(srcs_out)\n",
    "srcs = np.vstack(srcs_l)\n",
    "\n",
    "if len(srcs) == 0:\n",
    "    print('No sources detected, finishing script')\n",
    "    # continue ## No sources, continue\n",
    "\n",
    "print('Detected %d number of initial local maxima'%srcs.shape[0])\n",
    "\n",
    "srcs = srcs[np.argsort(srcs[:,3])]\n",
    "trv_out_srcs = trv(torch.Tensor(locs_use).to(device), torch.Tensor(srcs[:,0:3]).to(device)).cpu().detach() # .cpu().detach().numpy() # + srcs[:,3].reshape(-1,1,1)\n",
    "\n",
    "# print('Detected %d initial possible sources'%len(srcs))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3091d4-696b-46a0-893b-ac14ead36769",
   "metadata": {},
   "source": [
    "# Let's check, are these initial source locations reasonable? note: not final answer)\n",
    "<pre>\n",
    "Note: these are the GNN predicted locations, not the association + travel time residual minimization locations.\n",
    "\n",
    "These are also only detected over 3 hours (rather than the whole day)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40827e54-5564-4ff5-a245-f1390bedbf6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot sources\n",
    "fig, ax = plt.subplots(figsize = [8,8])\n",
    "plt.scatter(locs[:,1], locs[:,0], c = 'r', marker = '^')\n",
    "plt.scatter(srcs_known[:,1], srcs_known[:,0], c = 'C0', s = 6.5, marker = 'o', label = 'USGS')\n",
    "plt.scatter(srcs[:,1], srcs[:,0], c = 'C1', s = 6.5, marker = 'o', label = 'Ours')\n",
    "ax.set_aspect(1.0/np.cos(np.pi*locs[:,0].mean()/180.0)) ## Set projection aspect ratio\n",
    "d_pad = 0.5 ## Pad 0.5 degrees to plot axes\n",
    "ax.set_xlim(np.array(ax.get_xlim()) + np.array([-d_pad, d_pad]))\n",
    "ax.set_ylim(np.array(ax.get_ylim()) + np.array([-d_pad, d_pad]))\n",
    "plt.legend()\n",
    "plt.show(block = False)\n",
    "print('How is quality? What are issues?')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6249a70-0be9-4103-9cde-a62f55d0be01",
   "metadata": {},
   "source": [
    "## Now we'll run the part of the GNN that predicts *associations* for these events, and then locates them with travel time residual minimization (will take a few ~minutes to run)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3679c63-af63-44fe-8e14-86f23d6e1bdd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Run post processing detections.\n",
    "print('check the thresh assoc %f'%thresh_assoc)\n",
    "\n",
    "## Refine this\n",
    "\n",
    "n_segment = 1\n",
    "srcs_list = []\n",
    "n_intervals = int(np.floor(srcs.shape[0]/n_segment))\n",
    "\n",
    "for i in range(n_intervals):\n",
    "    srcs_list.append(np.arange(n_segment) + i*n_segment)\n",
    "\n",
    "if len(srcs_list) == 0:\n",
    "    srcs_list.append(np.arange(srcs.shape[0]))\n",
    "elif srcs_list[-1][-1] < (srcs.shape[0] - 1):\n",
    "    srcs_list.append(np.arange(srcs_list[-1][-1] + 1, srcs.shape[0]))\n",
    "\n",
    "## This section is memory intensive if lots of sources are detected.\n",
    "## Can \"loop\" over segements of sources, to keep the cost for manegable.\n",
    "\n",
    "print('Begin sources refined')\n",
    "srcs_refined_l = []\n",
    "trv_out_srcs_l = []\n",
    "Out_p_save_l = []\n",
    "Out_s_save_l = []\n",
    "\n",
    "Save_picks = [] # save all picks..\n",
    "lp_meta_l = []\n",
    "\n",
    "for n in range(len(srcs_list)):\n",
    "\n",
    "    Out_refined = []\n",
    "    X_query_1_list = []\n",
    "    X_query_1_cart_list = []\n",
    "\n",
    "    srcs_slice = srcs[srcs_list[n]]\n",
    "    trv_out_srcs_slice = trv_out_srcs[srcs_list[n]]\n",
    "\n",
    "    for i in range(srcs_slice.shape[0]):\n",
    "        # X_query = srcs[i,0:3] + X_offset\n",
    "        X_query_1 = srcs_slice[i,0:3] + (np.random.rand(n_rand_query,3)*(X_offset.max(0, keepdims = True) - X_offset.min(0, keepdims = True)) + X_offset.min(0, keepdims = True))\n",
    "        inside = np.where((X_query_1[:,0] > lat_range[0])*(X_query_1[:,0] < lat_range[1])*(X_query_1[:,1] > lon_range[0])*(X_query_1[:,1] < lon_range[1])*(X_query_1[:,2] > depth_range[0])*(X_query_1[:,2] < depth_range[1]))[0]\n",
    "        X_query_1 = X_query_1[inside]\n",
    "        X_query_1_cart = torch.Tensor(ftrns1(np.copy(X_query_1))).to(device) # \n",
    "        X_query_1_list.append(X_query_1)\n",
    "        X_query_1_cart_list.append(X_query_1_cart)\n",
    "\n",
    "        Out_refined.append(np.zeros((X_query_1.shape[0], len(tq))))\n",
    "\n",
    "    # with torch.no_grad(): \n",
    "    \n",
    "    for x_grid_ind in x_grid_ind_list_1:\n",
    "\n",
    "        if use_updated_input == False:\n",
    "        \n",
    "            [Inpts, Masks], [lp_times, lp_stations, lp_phases, lp_meta] = extract_inputs_from_data_fixed_grids_with_phase_type(trv, locs, ind_use, P, P[:,4], arrivals_tree, srcs_slice[:,3], x_grids[x_grid_ind], x_grids_trv[x_grid_ind], lat_range_extend, lon_range_extend, depth_range, max_t, training_params, graph_params, pred_params, ftrns1, ftrns2)\n",
    "\n",
    "        else:\n",
    "        \n",
    "            [Inpts, Masks], [lp_times, lp_stations, lp_phases, lp_meta] = extract_input_from_data(trv_pairwise, P, srcs_slice[:,3], ind_use, locs, x_grids[x_grid_ind], A_src_in_sta_l[x_grid_ind], trv_times = x_grids_trv[x_grid_ind], max_t = max_t, kernel_sig_t = pred_params[1], dt = dt_embed_discretize, device = device)\n",
    "        \n",
    "        # if use_subgraph == True:\n",
    "        # \tfor i in range(len(Inpts)):\n",
    "        #                 Inpts[i] = Inpts[i].reshape(x_grids[x_grid_ind].shape[0], locs_use.shape[0], 4)[A_src_in_sta_l[x_grid_ind][1], A_src_in_sta_l[x_grid_ind][0]] # .cpu().detach().numpy()\n",
    "        #                 Masks[i] = Masks[i].reshape(x_grids[x_grid_ind].shape[0], locs_use.shape[0], 4)[A_src_in_sta_l[x_grid_ind][1], A_src_in_sta_l[x_grid_ind][0]] # .cpu().detach().numpy()\n",
    "        \n",
    "        \n",
    "        if use_phase_types == False:\n",
    "            for i in range(len(Inpts)):\n",
    "                Inpts[i][:,2::] = 0.0 ## Phase type informed features zeroed out\n",
    "                Masks[i][:,2::] = 0.0\n",
    "    \n",
    "        for i in range(srcs_slice.shape[0]):\n",
    "\n",
    "            if len(lp_times[i]) == 0:\n",
    "                continue ## It will fail if len(lp_times[i0]) == 0!\n",
    "\n",
    "            ipick, tpick = lp_stations[i].astype('int'), lp_times[i] ## are these constant across different x_grid_ind?\n",
    "            \n",
    "            # note, trv_out_sources, is already on cuda, may cause memory issue with too many sources\n",
    "            out = mz_list[x_grid_ind].forward_fixed_source(torch.Tensor(Inpts[i]).to(device), torch.Tensor(Masks[i]).to(device), torch.Tensor(lp_times[i]).to(device), torch.Tensor(lp_stations[i]).long().to(device), torch.Tensor(lp_phases[i].reshape(-1,1)).float().to(device), torch.Tensor(ftrns1(locs_use)).to(device), x_grids_cart_torch[x_grid_ind], X_query_1_cart_list[i], tq)\n",
    "            Out_refined[i] += out[1][:,:,0].cpu().detach().numpy()/n_scale_x_grid_1\n",
    "\n",
    "    srcs_refined = []\n",
    "    for i in range(srcs_slice.shape[0]):\n",
    "\n",
    "        ip_argmax = np.argmax(Out_refined[i].max(1))\n",
    "        ipt_argmax = np.argmax(Out_refined[i][ip_argmax,:])\n",
    "        srcs_refined.append(np.concatenate((X_query_1_list[i][ip_argmax].reshape(1,-1), np.array([srcs_slice[i,3] + tq[ipt_argmax,0].item(), Out_refined[i].max()]).reshape(1,-1)), axis = 1)) \n",
    "\n",
    "    srcs_refined = np.vstack(srcs_refined)\n",
    "    srcs_refined = srcs_refined[np.argsort(srcs_refined[:,3])] # note, this\n",
    "\n",
    "    # re_apply_local_marching = True\n",
    "    # if re_apply_local_marching == True: ## This way, some events that were too far apart during initial LocalMarching\n",
    "    # \t## routine can now be grouped into one, since they are closer after the srcs_refined relocation step.\n",
    "    # \t## Note: ideally, this clustering should be done outside of the srcs_list loop, since nearby sources\n",
    "    # \t## may be artificically cut into seperate groups in srcs_list. Can end the srcs_list loop, run this\n",
    "    # \t## clustering, and then run the srcs_list group over the association results.\n",
    "    # \tmp = LocalMarching()\n",
    "    # \tsrcs_refined = mp(srcs_refined, ftrns1, tc_win = tc_win, sp_win = sp_win, scale_depth = scale_depth_clustering)\n",
    "    \n",
    "    ## Can do multiple grids simultaneously, for a single source? (by duplicating the source?)\n",
    "    trv_out_srcs_slice = trv(torch.Tensor(locs_use).to(device), torch.Tensor(srcs_refined[:,0:3]).to(device)).detach() # .cpu().detach().numpy() # + srcs[:,3].reshape(-1,1,1)\t\t\n",
    "\n",
    "    srcs_refined_l.append(srcs_refined)\n",
    "    trv_out_srcs_l.append(trv_out_srcs_slice)\n",
    "\n",
    "    ## Dense, spatial view.\n",
    "    d_deg = 0.1\n",
    "    x1 = np.arange(lat_range[0], lat_range[1] + d_deg, d_deg)\n",
    "    x2 = np.arange(lon_range[0], lon_range[1] + d_deg, d_deg)\n",
    "    x3 = np.array([0.0]) # This value is overwritten in the next step\n",
    "    x11, x12, x13 = np.meshgrid(x1, x2, x3)\n",
    "    xx = np.concatenate((x11.reshape(-1,1), x12.reshape(-1,1), x13.reshape(-1,1)), axis = 1)\n",
    "    X_save = np.copy(xx)[0,:].reshape(1,-1)\n",
    "    # X_save = X_save[0].reshape(1,-1) ## X_save is not actually used\n",
    "    X_save_cart = torch.Tensor(ftrns1(X_save))\n",
    "\n",
    "    # with torch.no_grad(): \n",
    "    \n",
    "    for inc, x_grid_ind in enumerate(x_grid_ind_list_1):\n",
    "\n",
    "        if use_updated_input == False:\n",
    "        \n",
    "            [Inpts, Masks], [lp_times, lp_stations, lp_phases, lp_meta] = extract_inputs_from_data_fixed_grids_with_phase_type(trv, locs, ind_use, P, P[:,4], arrivals_tree, srcs_refined[:,3], x_grids[x_grid_ind], x_grids_trv[x_grid_ind], lat_range_extend, lon_range_extend, depth_range, max_t, training_params, graph_params, pred_params, ftrns1, ftrns2)\n",
    "\n",
    "        else:\n",
    "\n",
    "            [Inpts, Masks], [lp_times, lp_stations, lp_phases, lp_meta] = extract_input_from_data(trv_pairwise, P, srcs_refined[:,3], ind_use, locs, x_grids[x_grid_ind], A_src_in_sta_l[x_grid_ind], trv_times = x_grids_trv[x_grid_ind], max_t = max_t, kernel_sig_t = pred_params[1], dt = dt_embed_discretize, device = device)\t\t\t\t\n",
    "\n",
    "        # if use_subgraph == True:\n",
    "        # \tfor i in range(len(Inpts)):\n",
    "        #                 Inpts[i] = Inpts[i].reshape(x_grids[x_grid_ind].shape[0], locs_use.shape[0], 4)[A_src_in_sta_l[x_grid_ind][1], A_src_in_sta_l[x_grid_ind][0]] # .cpu().detach().numpy()\n",
    "        #                 Masks[i] = Masks[i].reshape(x_grids[x_grid_ind].shape[0], locs_use.shape[0], 4)[A_src_in_sta_l[x_grid_ind][1], A_src_in_sta_l[x_grid_ind][0]] # .cpu().detach().numpy()\n",
    "\n",
    "        \n",
    "        if use_phase_types == False:\n",
    "            for i in range(len(Inpts)):\n",
    "                Inpts[i][:,2::] = 0.0 ## Phase type informed features zeroed out\n",
    "                Masks[i][:,2::] = 0.0\n",
    "        \n",
    "        if inc == 0:\n",
    "\n",
    "            Out_p_save = [np.zeros(len(lp_times[j])) for j in range(srcs_refined.shape[0])]\n",
    "            Out_s_save = [np.zeros(len(lp_times[j])) for j in range(srcs_refined.shape[0])]\n",
    "\n",
    "        for i in range(srcs_refined.shape[0]):\n",
    "\n",
    "            # Does this cause any issues? Could each ipick, tpick, not be constant, between grids?\n",
    "            ipick, tpick = lp_stations[i].astype('int'), lp_times[i]\n",
    "\n",
    "            if inc == 0:\n",
    "\n",
    "                Save_picks.append(np.concatenate((tpick.reshape(-1,1), ipick.reshape(-1,1)), axis = 1))\n",
    "                lp_meta_l.append(lp_meta[i])\n",
    "\n",
    "            X_save[:,2] = srcs_refined[i,2]\n",
    "            X_save_cart = torch.Tensor(ftrns1(X_save)).to(device)\n",
    "\n",
    "            if len(lp_times[i]) == 0:\n",
    "                continue ## It will fail if len(lp_times[i0]) == 0!\t\t\t\t\n",
    "            \n",
    "            out = mz_list[x_grid_ind].forward_fixed(torch.Tensor(Inpts[i]).to(device), torch.Tensor(Masks[i]).to(device), torch.Tensor(lp_times[i]).to(device), torch.Tensor(lp_stations[i]).long().to(device), torch.Tensor(lp_phases[i].reshape(-1,1)).long().to(device), torch.Tensor(ftrns1(locs_use)).to(device), x_grids_cart_torch[x_grid_ind], X_save_cart, torch.Tensor(ftrns1(srcs_refined[i,0:3].reshape(1,-1))).to(device), tq, torch.zeros(1).to(device), trv_out_srcs_slice[[i],:,:])\n",
    "            # Out_save[i,:,:] += out[1][:,:,0].cpu().detach().numpy()/n_scale_x_grid_1\n",
    "            Out_p_save[i] += out[2][0,:,0].cpu().detach().numpy()/n_scale_x_grid_1\n",
    "            Out_s_save[i] += out[3][0,:,0].cpu().detach().numpy()/n_scale_x_grid_1\n",
    "\n",
    "    for i in range(srcs_refined.shape[0]):\n",
    "        Out_p_save_l.append(Out_p_save[i])\n",
    "        Out_s_save_l.append(Out_s_save[i])\n",
    "\n",
    "\n",
    "\n",
    "srcs_refined = np.vstack(srcs_refined_l)\n",
    "\n",
    "mp = LocalMarching(device = device)\n",
    "srcs_refined_1 = mp(srcs_refined, ftrns1, tc_win = tc_win, sp_win = sp_win, scale_depth = scale_depth_clustering)\n",
    "\n",
    "tree_refined = cKDTree(ftrns1(srcs_refined))\n",
    "ip_retained = tree_refined.query(ftrns1(srcs_refined_1))[1]\n",
    "\n",
    "Out_p_save = [Out_p_save_l[i] for i in ip_retained]\n",
    "Out_s_save = [Out_s_save_l[i] for i in ip_retained]\n",
    "lp_meta_l = [lp_meta_l[i] for i in ip_retained]\n",
    "Save_picks = [Save_picks[i] for i in ip_retained]\n",
    "srcs_refined = srcs_refined[ip_retained]\n",
    "\n",
    "trv_out_srcs = trv(torch.Tensor(locs_use).to(device), torch.Tensor(srcs_refined[:,0:3]).to(device)).cpu().detach()\n",
    "\n",
    "print('Begin competetive assignment')\n",
    "iargsort = np.argsort(srcs_refined[:,3])\n",
    "srcs_refined = srcs_refined[iargsort]\n",
    "trv_out_srcs = trv_out_srcs[iargsort]\n",
    "Out_p_save = [Out_p_save[i] for i in iargsort]\n",
    "Out_s_save = [Out_s_save[i] for i in iargsort]\n",
    "Save_picks = [Save_picks[i] for i in iargsort]\n",
    "lp_meta = [lp_meta_l[i] for i in iargsort]\n",
    "\n",
    "if (use_expanded_competitive_assignment == False) or (len(srcs_refined) <= 1):\n",
    "\n",
    "    Assigned_picks = []\n",
    "    Picks_P = []\n",
    "    Picks_S = []\n",
    "    Picks_P_perm = []\n",
    "    Picks_S_perm = []\n",
    "    # Out_save = []\n",
    "\n",
    "    ## Implement CA, so that is runs over disjoint sets of \"nearby\" sources.\n",
    "    ## Rather than individually, for each source.\n",
    "    for i in range(srcs_refined.shape[0]):\n",
    "\n",
    "        ## Now do assignments, on the stacked association predictions (over grids)\n",
    "\n",
    "        ipick, tpick = Save_picks[i][:,1].astype('int'), Save_picks[i][:,0]\n",
    "\n",
    "        print(i)\n",
    "\n",
    "        ## Need to replace this with competitive assignment over \"connected\"\n",
    "        ## Sources. This will reduce duplicate events.\n",
    "        wp = np.zeros((1,len(tpick))); wp[0,:] = Out_p_save[i]\n",
    "        ws = np.zeros((1,len(tpick))); ws[0,:] = Out_s_save[i]\n",
    "        wp[wp <= thresh_assoc] = 0.0\n",
    "        ws[ws <= thresh_assoc] = 0.0\n",
    "        assignments, srcs_active = competitive_assignment([wp, ws], ipick, 1.5, force_n_sources = 1) ## force 1 source?\n",
    "        \n",
    "\n",
    "        # Note, calling tree_picks\n",
    "        ip_picks = tree_picks.query(lp_meta[i][:,0:2]) # meta uses absolute indices\n",
    "        assert(abs(ip_picks[0]).max() == 0.0)\n",
    "        ip_picks = ip_picks[1]\n",
    "\n",
    "        # p_pred, s_pred = np.zeros(len(tpick)), np.zeros(len(tpick))\n",
    "        assert(len(srcs_active) == 1)\n",
    "        ## Assumes 1 source\n",
    "\n",
    "        ind_p = ipick[assignments[0][0]]\n",
    "        ind_s = ipick[assignments[0][1]]\n",
    "        arv_p = tpick[assignments[0][0]]\n",
    "        arv_s = tpick[assignments[0][1]]\n",
    "\n",
    "        p_assign = np.concatenate((P[ip_picks[assignments[0][0]],:], i*np.ones(len(assignments[0][0])).reshape(-1,1)), axis = 1) ## Note: could concatenate ip_picks, if desired here, so all picks in Picks_P lists know the index of the absolute pick index.\n",
    "        s_assign = np.concatenate((P[ip_picks[assignments[0][1]],:], i*np.ones(len(assignments[0][1])).reshape(-1,1)), axis = 1)\n",
    "        p_assign_perm = np.copy(p_assign)\n",
    "        s_assign_perm = np.copy(s_assign)\n",
    "        p_assign_perm[:,1] = perm_vec[p_assign_perm[:,1].astype('int')]\n",
    "        s_assign_perm[:,1] = perm_vec[s_assign_perm[:,1].astype('int')]\n",
    "        Picks_P.append(p_assign)\n",
    "        Picks_S.append(s_assign)\n",
    "        Picks_P_perm.append(p_assign_perm)\n",
    "        Picks_S_perm.append(s_assign_perm)\n",
    "\n",
    "        print('add relocation!')\n",
    "\n",
    "        ## Implemente CA, to deal with mixing events (nearby in time, with shared arrival association assignments)\n",
    "\n",
    "elif use_expanded_competitive_assignment == True:\n",
    "\n",
    "    Assigned_picks = []\n",
    "    Picks_P = []\n",
    "    Picks_S = []\n",
    "    Picks_P_perm = []\n",
    "    Picks_S_perm = []\n",
    "    # Out_save = []\n",
    "\n",
    "    ## Implement CA, so that is runs over disjoint sets of \"nearby\" sources.\n",
    "    ## Rather than individually, for each source.\n",
    "\n",
    "    # ## Find overlapping events (events with shared pick assignments)\n",
    "    all_picks = np.vstack(lp_meta) # [:,0:2] # np.vstack([Save_picks[i] for i in range(len(Save_picks))])\n",
    "    # unique_picks = np.unique(all_picks[:,0:2], axis = 0)\n",
    "    unique_picks = np.unique(all_picks, axis = 0)\n",
    "\n",
    "    # ip_sort_unique = np.lexsort((unique_picks[:,0], unique_picks[:,1])) # sort by station\n",
    "    ip_sort_unique = np.lexsort((unique_picks[:,1], unique_picks[:,0])) # sort by time\n",
    "    unique_picks = unique_picks[ip_sort_unique]\n",
    "    len_unique_picks = len(unique_picks)\n",
    "\n",
    "    # tree_picks_select = cKDTree(all_picks[:,0:2])\n",
    "    tree_picks_unique_select = cKDTree(unique_picks[:,0:2])\n",
    "    # lp_tree_picks_select  = tree_picks_select.query_ball_point(unique_picks, r = 0)\n",
    "\n",
    "    matched_src_arrival_indices = []\n",
    "    matched_src_arrival_indices_p = []\n",
    "    matched_src_arrival_indices_s = []\n",
    "\n",
    "    min_picks = 4\n",
    "\n",
    "    for i in range(len(lp_meta)):\n",
    "\n",
    "        if len(lp_meta[i]) == 0:\n",
    "            continue\n",
    "\n",
    "        matched_arv_indices_val = tree_picks_unique_select.query(lp_meta[i][:,0:2])\n",
    "        assert(matched_arv_indices_val[0].max() == 0)\n",
    "        matched_arv_indices = matched_arv_indices_val[1]\n",
    "\n",
    "        ifind_p = np.where(Out_p_save[i] > thresh_assoc)[0]\n",
    "        ifind_s = np.where(Out_s_save[i] > thresh_assoc)[0]\n",
    "\n",
    "        # Check for minimum number of picks, otherwise, skip source\n",
    "        if (len(ifind_p) + len(ifind_s)) >= min_picks:\n",
    "\n",
    "            ifind = np.unique(np.concatenate((ifind_p, ifind_s), axis = 0)) # Create combined set of indices\n",
    "\n",
    "            ## concatenate both p and s likelihoods and edges for all of ifind, so that the dense matrices extracted for each\n",
    "            ## disconnected component are the same size.\n",
    "\n",
    "            ## First row is arrival indices, second row are src indices\n",
    "            # if len(ifind_p) > 0:\n",
    "            # matched_src_arrival_indices_p.append(np.concatenate((matched_arv_indices[ifind_p].reshape(1,-1), i*np.ones(len(ifind_p)).reshape(1,-1), Out_p_save[i][ifind_p].reshape(1,-1)), axis = 0))\n",
    "            matched_src_arrival_indices_p.append(np.concatenate((matched_arv_indices[ifind].reshape(1,-1), i*np.ones(len(ifind)).reshape(1,-1), Out_p_save[i][ifind].reshape(1,-1)), axis = 0))\n",
    "\n",
    "            # if len(ifind_s) > 0:\n",
    "            # matched_src_arrival_indices_s.append(np.concatenate((matched_arv_indices[ifind_s].reshape(1,-1), i*np.ones(len(ifind_s)).reshape(1,-1), Out_s_save[i][ifind_s].reshape(1,-1)), axis = 0))\n",
    "            matched_src_arrival_indices_s.append(np.concatenate((matched_arv_indices[ifind].reshape(1,-1), i*np.ones(len(ifind)).reshape(1,-1), Out_s_save[i][ifind].reshape(1,-1)), axis = 0))\n",
    "\n",
    "            matched_src_arrival_indices.append(np.concatenate((matched_arv_indices[ifind].reshape(1,-1), i*np.ones(len(ifind)).reshape(1,-1), np.concatenate((Out_p_save[i][ifind].reshape(1,-1), Out_s_save[i][ifind].reshape(1,-1)), axis = 0).max(0, keepdims = True)), axis = 0))\n",
    "\n",
    "    ## From this, we may not have memory issues with competitive assignment. If so,\n",
    "    ## can still reduce the size of disjoint groups.\n",
    "\n",
    "    matched_src_arrival_indices = np.hstack(matched_src_arrival_indices)\n",
    "    matched_src_arrival_indices_p = np.hstack(matched_src_arrival_indices_p)\n",
    "    matched_src_arrival_indices_s = np.hstack(matched_src_arrival_indices_s)\n",
    "\n",
    "    ## Convert to linear graph, find disconected components, apply CA\n",
    "\n",
    "    w_edges = np.concatenate((matched_src_arrival_indices[0,:][None,:], matched_src_arrival_indices[1,:][None,:] + len_unique_picks, matched_src_arrival_indices[2,:].reshape(1,-1)), axis = 0)\n",
    "    wp_edges = np.concatenate((matched_src_arrival_indices_p[0,:][None,:], matched_src_arrival_indices_p[1,:][None,:] + len_unique_picks, matched_src_arrival_indices_p[2,:].reshape(1,-1)), axis = 0)\n",
    "    ws_edges = np.concatenate((matched_src_arrival_indices_s[0,:][None,:], matched_src_arrival_indices_s[1,:][None,:] + len_unique_picks, matched_src_arrival_indices_s[2,:].reshape(1,-1)), axis = 0)\n",
    "    assert(np.abs(wp_edges[0:2,:] - ws_edges[0:2,:]).max() == 0)\n",
    "\n",
    "    ## w_edges: first row are unique arrival indices\n",
    "    ## w_edges: second row are unique src indices (with index 0 being the len(unique_picks))\n",
    "\n",
    "    ## Need to combined wp and ws graphs\n",
    "    G_nx = nx.Graph()\n",
    "    G_nx.add_weighted_edges_from(w_edges.T)\n",
    "    G_nx.add_weighted_edges_from(w_edges[np.array([1,0,2]),:].T)\n",
    "\n",
    "    Gp_nx = nx.Graph()\n",
    "    Gp_nx.add_weighted_edges_from(wp_edges.T)\n",
    "    Gp_nx.add_weighted_edges_from(wp_edges[np.array([1,0,2]),:].T)\n",
    "\n",
    "    Gs_nx = nx.Graph()\n",
    "    Gs_nx.add_weighted_edges_from(ws_edges.T)\n",
    "    Gs_nx.add_weighted_edges_from(ws_edges[np.array([1,0,2]),:].T)\n",
    "\n",
    "    discon_components = list(nx.connected_components(G_nx))\n",
    "    discon_components = [np.sort(np.array(list(discon_components[i])).astype('int')) for i in range(len(discon_components))]\n",
    "\n",
    "    finish_splits = False\n",
    "    max_sources = 15 ## per competitive assignment run\n",
    "    max_splits = 30\n",
    "    num_splits = 0\n",
    "    while finish_splits == False:\n",
    "\n",
    "        remove_edges_from = []\n",
    "\n",
    "        discon_components = list(nx.connected_components(G_nx))\n",
    "        discon_components = [np.sort(np.array(list(discon_components[i])).astype('int')) for i in range(len(discon_components))]\n",
    "\n",
    "        ## Should the below line really use a where function? It seems like this is a \"where\" on a scalar velue everytime, so it is guarenteed to evaluate as 1\n",
    "        len_discon = np.array([len(np.where(discon_components[j] > (len_unique_picks - 1))[0]) for j in range(len(discon_components))])\n",
    "        print('Number discon components: %d \\n'%(len(len_discon)))\n",
    "        print('Number large discon components: %d \\n'%(len(np.where(len_discon > max_sources)[0])))\n",
    "        print('Largest discon component: %d \\n'%(max(len_discon)))\n",
    "\n",
    "        if (len(np.where(len_discon > max_sources)[0]) == 0) or (num_splits > max_splits):\n",
    "            finish_splits = True\n",
    "            continue\n",
    "\n",
    "        print('Beginning split step %d'%num_splits)\n",
    "\n",
    "        for i in range(len(discon_components)):\n",
    "\n",
    "            subset_edges = G_nx.subgraph(discon_components[i])\n",
    "            adj_matrix = nx.adjacency_matrix(subset_edges, nodelist = discon_components[i]).toarray() # nodelist = np.arange(len(discon_components[i]))).toarray()\n",
    "\n",
    "            subset_edges = Gp_nx.subgraph(discon_components[i])\n",
    "            adj_matrix_p = nx.adjacency_matrix(subset_edges, nodelist = discon_components[i]).toarray() # nodelist = np.arange(len(discon_components[i]))).toarray()\n",
    "\n",
    "            subset_edges = Gs_nx.subgraph(discon_components[i])\n",
    "            adj_matrix_s = nx.adjacency_matrix(subset_edges, nodelist = discon_components[i]).toarray() # nodelist = np.arange(len(discon_components[i]))).toarray()\n",
    "\n",
    "            # ifind_matched_inds = tree_srcs.query(w_edges[0:2,discon_components[i]].T)[1]\n",
    "\n",
    "            ## Apply CA to the subset of sources/picks in a disconnected component\n",
    "            ifind_src_inds = np.where(discon_components[i] > (len_unique_picks - 1))[0]\n",
    "            ifind_arv_inds = np.delete(np.arange(len(discon_components[i])), ifind_src_inds, axis = 0)\n",
    "\n",
    "            arv_ind_slice = np.sort(discon_components[i][ifind_arv_inds])\n",
    "            arv_src_slice = np.sort(discon_components[i][ifind_src_inds]) - len_unique_picks\n",
    "            len_arv_slice = len(arv_ind_slice)\n",
    "\n",
    "            tpick = unique_picks[arv_ind_slice,0]\n",
    "            ipick = unique_picks[arv_ind_slice,1].astype('int')\n",
    "\n",
    "            if len(ifind_src_inds) <= max_sources:\n",
    "\n",
    "                pass\n",
    "\n",
    "            elif len(ifind_src_inds) > max_sources:\n",
    "\n",
    "                ## Create a source-source index graph, based on how much they \"share\" arrivals. Then find min-cut on this graph,\n",
    "                ## to seperate sources. Modify the discon_components so the sources are split.\n",
    "\n",
    "                w_slice = adj_matrix[len_arv_slice::,0:len_arv_slice] # np.zeros((len(arv_src_slice), len(arv_ind_slice)))\n",
    "                wp_slice = adj_matrix_p[len_arv_slice::,0:len_arv_slice] # np.zeros((len(arv_src_slice), len(arv_ind_slice)))\n",
    "                ws_slice = adj_matrix_s[len_arv_slice::,0:len_arv_slice] # np.zeros((len(arv_src_slice), len(arv_ind_slice)))\n",
    "\n",
    "                isource, iarv = np.where(w_slice > thresh_assoc)\n",
    "                tree_src_ind = cKDTree(isource.reshape(-1,1)) ## all sources should appear here\n",
    "                # lp_src_ind = tree_src_ind.query_ball_point(np.sort(np.unique(isource)).reshape(-1,1), r = 0)\n",
    "                lp_src_ind = tree_src_ind.query_ball_point(np.arange(len(ifind_src_inds)).reshape(-1,1), r = 0)\n",
    "\n",
    "                assert(len(np.sort(np.unique(isource))) == len(ifind_src_inds))\n",
    "\n",
    "                ## Note: could concievably use MCL on these graphs, just like in original association application.\n",
    "                ## May want to use MCL even on the original source-time graphs as well.\n",
    "                \n",
    "                w_src_adj = np.zeros((len(ifind_src_inds), len(ifind_src_inds)))\n",
    "\n",
    "                for j in range(len(ifind_src_inds)):\n",
    "                    for k in range(len(ifind_src_inds)):\n",
    "                        if j == k:\n",
    "                            continue\n",
    "                        if (len(lp_src_ind[j]) > 0)*(len(lp_src_ind[k]) > 0):\n",
    "                            w_src_adj[j,k] = len(list(set(iarv[lp_src_ind[j]]).intersection(iarv[lp_src_ind[k]])))\n",
    "\n",
    "                ## Simply split sources into groups of two (need to make sure this rarely cuts off indidual sources)\n",
    "                clusters = SpectralClustering(n_clusters = 2, affinity = 'precomputed').fit_predict(w_src_adj)\n",
    "\n",
    "                i1, i2 = np.where(clusters == 0)[0], np.where(clusters == 1)[0]\n",
    "\n",
    "                ## Optimize all (union) of picks between split sources, so can determine which edges (between arrivals and sources) to delete\n",
    "                ## This should `trim' the source-arrival graphs and increase amount of disconnected components.\n",
    "\n",
    "                # min_time1, min_time2 = srcs_refined[ifind_src_inds[i1],3].min(), srcs_refined[ifind_src_inds[i2],3].min()\n",
    "                min_time1, min_time2 = srcs_refined[arv_src_slice[i1],3].min(), srcs_refined[arv_src_slice[i2],3].min()\n",
    "\n",
    "                if min_time1 <= min_time2:\n",
    "                    # cutset = nx.minimum_edge_cut(g_src, s = max(i1), t = min(i2))\n",
    "                    pass\n",
    "                else:\n",
    "                    i3 = np.copy(i1)\n",
    "                    i1 = np.copy(i2)\n",
    "                    i2 = np.copy(i3)\n",
    "\n",
    "                ## Instead of cut-set, find all sources that \"link\" across the two groups. Use these as reference sources.\n",
    "                ## In bad cases, could this set also be too big?\n",
    "                cutset_left = []\n",
    "                cutset_right = []\n",
    "                for j in range(len(i1)):\n",
    "                    cutset_right.append(i2[np.where(w_src_adj[i1[j],i2] > 0)[0]])\n",
    "                for j in range(len(i2)):\n",
    "                    cutset_left.append(i1[np.where(w_src_adj[i2[j],i1] > 0)[0]])\n",
    "\n",
    "                cutset_left = np.unique(np.hstack(cutset_left))\t\n",
    "                cutset_right = np.unique(np.hstack(cutset_right))\t\n",
    "                cutset = np.unique(np.concatenate((cutset_left, cutset_right), axis = 0))\n",
    "\n",
    "                ## Extract the arrival-source weights from w_edges for these nodes\n",
    "                ## Then \"take max value\" of these picks across these sources\n",
    "                ## Then use CA to maximize assignment of picks to either \"distinct\"\n",
    "                ## cluster. Then remove those arrival attachements from the full graph\n",
    "                ## for the cluster the picks arn't assigned too. Then, do this for all\n",
    "                ## disconnected graphs, update the disconnected components, and iterate\n",
    "                ## until all graphs are less than or equal to maximum size.\n",
    "\n",
    "                # cutset = np.array(list(cutset)).astype('int')\n",
    "                unique_src_inds = np.sort(np.unique(cutset.reshape(-1,1))).astype('int')\n",
    "                arv_indices_sliced = np.where(w_slice[unique_src_inds,:].max(0) > thresh_assoc)[0]\n",
    "\n",
    "                arv_weights_p_cluster_1 = wp_slice[np.unique(cutset_left).astype('int').reshape(-1,1), arv_indices_sliced.reshape(1,-1)].max(0).reshape(1,-1)\n",
    "                arv_weights_s_cluster_1 = ws_slice[np.unique(cutset_left).astype('int').reshape(-1,1), arv_indices_sliced.reshape(1,-1)].max(0).reshape(1,-1)\n",
    "\n",
    "                arv_weights_p_cluster_2 = wp_slice[np.unique(cutset_right).astype('int').reshape(-1,1), arv_indices_sliced.reshape(1,-1)].max(0).reshape(1,-1)\n",
    "                arv_weights_s_cluster_2 = ws_slice[np.unique(cutset_right).astype('int').reshape(-1,1), arv_indices_sliced.reshape(1,-1)].max(0).reshape(1,-1)\n",
    "\n",
    "                arv_weights_p = np.concatenate((arv_weights_p_cluster_1, arv_weights_p_cluster_2), axis = 0)\n",
    "                arv_weights_s = np.concatenate((arv_weights_s_cluster_1, arv_weights_s_cluster_2), axis = 0)\n",
    "\n",
    "                ## Now: use competitive assignment to optimize pick assignments to either cluster (use a cost on sources, or no?)\n",
    "                # assignment_picks, srcs_active_picks = competitive_assignment_split([arv_weights_p, arv_weights_s], ipick[arv_indices_sliced], 1.0) ## force 1 source?\n",
    "                assignment_picks, srcs_active_picks = competitive_assignment_split([arv_weights_p, arv_weights_s], ipick[arv_indices_sliced], 0.0) ## force 1 source?\n",
    "                node_all_arrivals = arv_ind_slice[arv_indices_sliced]\n",
    "\n",
    "                if len(assignment_picks) > 0:\n",
    "                    assign_picks_1 = np.unique(np.hstack(assignment_picks[0]))\n",
    "                else:\n",
    "                    assign_picks_1 = np.array([])\n",
    "\n",
    "                ## Cut these arrivals from sources in group 1\n",
    "                node_src_1 = arv_src_slice[cutset_left] + len_unique_picks\n",
    "                node_arrival_1_del = np.delete(node_all_arrivals, assign_picks_1, axis = 0)\n",
    "                node_arrival_1_repeat = np.repeat(node_arrival_1_del, len(node_src_1), axis = 0)\n",
    "                node_src_1_repeat = np.tile(node_src_1, len(node_arrival_1_del))\n",
    "                remove_edges_from.append(np.concatenate((node_arrival_1_repeat.reshape(1,-1), node_src_1_repeat.reshape(1,-1)), axis = 0))\n",
    "\n",
    "                if len(assignment_picks) > 1:\n",
    "                    assign_picks_2 = np.unique(np.hstack(assignment_picks[1]))\n",
    "                    # node_arrival_2 = arv_ind_slice[arv_indices_sliced[assign_picks_2]]\n",
    "                else:\n",
    "                    # node_arrival_2 = np.array([])\n",
    "                    assign_picks_2 = np.array([])\n",
    "\n",
    "                node_src_2 = arv_src_slice[cutset_right] + len_unique_picks\n",
    "                node_arrival_2_del = np.delete(node_all_arrivals, assign_picks_2, axis = 0)\n",
    "                node_arrival_2_repeat = np.repeat(node_arrival_2_del, len(node_src_2), axis = 0)\n",
    "                node_src_2_repeat = np.tile(node_src_2, len(node_arrival_2_del))\n",
    "                remove_edges_from.append(np.concatenate((node_arrival_2_repeat.reshape(1,-1), node_src_2_repeat.reshape(1,-1)), axis = 0))\n",
    "\n",
    "                print('%d %d %d'%(len(arv_ind_slice), sum(clusters == 0), sum(clusters == 1)))\n",
    "\n",
    "        if len(remove_edges_from) > 0:\n",
    "            remove_edges_from = np.hstack(remove_edges_from)\n",
    "            remove_edges_from = np.concatenate((remove_edges_from, np.flip(remove_edges_from, axis = 0)), axis = 1)\n",
    "\n",
    "            G_nx.remove_edges_from(remove_edges_from.T)\n",
    "            Gp_nx.remove_edges_from(remove_edges_from.T)\n",
    "            Gs_nx.remove_edges_from(remove_edges_from.T)\n",
    "\n",
    "        num_splits = num_splits + 1\n",
    "\n",
    "    srcs_retained = []\n",
    "    cnt_src = 0\n",
    "\n",
    "    for i in range(len(discon_components)):\n",
    "\n",
    "        ## Need to check that each subgraph and sets of edges are for same combinations of source-arrivals,\n",
    "        ## for all three graphs.\n",
    "\n",
    "        subset_edges = G_nx.subgraph(discon_components[i])\n",
    "        adj_matrix = nx.adjacency_matrix(subset_edges, nodelist = discon_components[i]).toarray() # nodelist = np.arange(len(discon_components[i]))).toarray()\n",
    "\n",
    "        subset_edges = Gp_nx.subgraph(discon_components[i])\n",
    "        adj_matrix_p = nx.adjacency_matrix(subset_edges, nodelist = discon_components[i]).toarray() # nodelist = np.arange(len(discon_components[i]))).toarray()\n",
    "\n",
    "        subset_edges = Gs_nx.subgraph(discon_components[i])\n",
    "        adj_matrix_s = nx.adjacency_matrix(subset_edges, nodelist = discon_components[i]).toarray() # nodelist = np.arange(len(discon_components[i]))).toarray()\n",
    "\n",
    "        ## Apply CA to the subset of sources/picks in a disconnected component\n",
    "        ifind_src_inds = np.where(discon_components[i] > (len_unique_picks - 1))[0]\n",
    "        ifind_arv_inds = np.delete(np.arange(len(discon_components[i])), ifind_src_inds, axis = 0)\n",
    "\n",
    "        arv_ind_slice = np.sort(discon_components[i][ifind_arv_inds])\n",
    "        arv_src_slice = np.sort(discon_components[i][ifind_src_inds]) - len_unique_picks\n",
    "        len_arv_slice = len(arv_ind_slice)\n",
    "\n",
    "        wp_slice = adj_matrix_p[len_arv_slice::,0:len_arv_slice] # np.zeros((len(arv_src_slice), len(arv_ind_slice)))\n",
    "        ws_slice = adj_matrix_s[len_arv_slice::,0:len_arv_slice] # np.zeros((len(arv_src_slice), len(arv_ind_slice)))\n",
    "        \n",
    "        tpick = unique_picks[arv_ind_slice,0]\n",
    "        ipick = unique_picks[arv_ind_slice,1].astype('int')\n",
    "\n",
    "        ## Now do assignments, on the stacked association predictions (over grids)\n",
    "\n",
    "        if (len(ipick) == 0) or (len(arv_src_slice) == 0):\n",
    "            continue\n",
    "\n",
    "        # thresh_assoc = 0.125\n",
    "        wp_slice[wp_slice <= thresh_assoc] = 0.0\n",
    "        ws_slice[ws_slice <= thresh_assoc] = 0.0\n",
    "        # assignments, srcs_active = competitive_assignment([wp_slice, ws_slice], ipick, 1.5, force_n_sources = 1) ## force 1 source?\n",
    "        assignments, srcs_active = competitive_assignment([wp_slice, ws_slice], ipick, cost_value) ## force 1 source?\n",
    "\n",
    "        if len(srcs_active) > 0:\n",
    "\n",
    "            for j in range(len(srcs_active)):\n",
    "\n",
    "\n",
    "                srcs_retained.append(srcs_refined[arv_src_slice[srcs_active[j]]].reshape(1,-1))\n",
    "\n",
    "                wp_val = wp_slice[srcs_active[j], assignments[j][0]]\n",
    "                ws_val = ws_slice[srcs_active[j], assignments[j][1]]\n",
    "                \n",
    "                p_assign = np.concatenate((unique_picks[arv_ind_slice[assignments[j][0]],:], cnt_src*np.ones(len(assignments[j][0])).reshape(-1,1), wp_val.reshape(-1,1)), axis = 1) ## Note: could concatenate ip_picks, if desired here, so all picks in Picks_P lists know the index of the absolute pick index.\n",
    "                s_assign = np.concatenate((unique_picks[arv_ind_slice[assignments[j][1]],:], cnt_src*np.ones(len(assignments[j][1])).reshape(-1,1), ws_val.reshape(-1,1)), axis = 1)\n",
    "                p_assign_perm = np.copy(p_assign)\n",
    "                s_assign_perm = np.copy(s_assign)\n",
    "                p_assign_perm[:,1] = perm_vec[p_assign_perm[:,1].astype('int')]\n",
    "                s_assign_perm[:,1] = perm_vec[s_assign_perm[:,1].astype('int')]\n",
    "                Picks_P.append(p_assign)\n",
    "                Picks_S.append(s_assign)\n",
    "                Picks_P_perm.append(p_assign_perm)\n",
    "                Picks_S_perm.append(s_assign_perm)\n",
    "\n",
    "                cnt_src += 1\n",
    "\n",
    "        print('%d : %d of %d'%(i, len(srcs_active), len(arv_src_slice)))\n",
    "\n",
    "        ## Find unique set of arrival indices, write to subset of matrix weights\n",
    "        ## for wp and ws.\n",
    "\n",
    "        ## Then solve CA. Need to scale weights so that: (i). Primarily, the cost is related to the number\n",
    "        ## of picks per event, and (ii). It still identifies \"good\" fit and \"bad\" fit source-arrival pairs,\n",
    "        ## based on the source-arrival weights.\n",
    "\n",
    "    if len(srcs_retained) == 0:\n",
    "        print('No events left after competitive assignment (e.g., cost is too high w.r.t. amount of available picks)')\n",
    "        # continue\n",
    "    \n",
    "    srcs_refined = np.vstack(srcs_retained)\n",
    "\n",
    "\n",
    "## Locate events using travel times and associated picks\n",
    "srcs_trv, srcs_sigma = [], []\n",
    "del_arv_p, del_arv_s = [], []\n",
    "torch.set_grad_enabled(True)\n",
    "for i in range(srcs_refined.shape[0]):\n",
    "\n",
    "    arv_p, ind_p, arv_s, ind_s = Picks_P_perm[i][:,0], Picks_P_perm[i][:,1].astype('int'), Picks_S_perm[i][:,0], Picks_S_perm[i][:,1].astype('int')\n",
    "\n",
    "    ind_unique_arrivals = np.sort(np.unique(np.concatenate((ind_p, ind_s), axis = 0)).astype('int'))\n",
    "\n",
    "    if len(ind_unique_arrivals) == 0:\n",
    "        srcs_trv.append(np.nan*np.ones((1, 4)))\n",
    "        srcs_sigma.append(np.nan)\n",
    "        del_arv_p.append(0)\n",
    "        del_arv_s.append(0)\n",
    "        continue\t\t\t\n",
    "    \n",
    "    perm_vec_arrivals = -1*np.ones(locs_use.shape[0]).astype('int')\n",
    "    perm_vec_arrivals[ind_unique_arrivals] = np.arange(len(ind_unique_arrivals))\n",
    "    locs_use_slice = locs_use[ind_unique_arrivals]\n",
    "    ind_p_perm_slice = perm_vec_arrivals[ind_p]\n",
    "    ind_s_perm_slice = perm_vec_arrivals[ind_s]\n",
    "    if len(ind_p_perm_slice) > 0:\n",
    "        assert(ind_p_perm_slice.min() > -1)\n",
    "    if len(ind_s_perm_slice) > 0:\n",
    "        assert(ind_s_perm_slice.min() > -1)\n",
    "\n",
    "    if use_differential_evolution_location == True:\n",
    "        xmle, logprob = differential_evolution_location(trv, locs_use_slice, arv_p, ind_p_perm_slice, arv_s, ind_s_perm_slice, lat_range_extend, lon_range_extend, depth_range, surface_profile = surface_profile, device = device)\n",
    "    else:\n",
    "        xmle, logprob, Swarm = MLE_particle_swarm_location_one_mean_stable_depth_with_hull(trv, locs_use_slice, arv_p, ind_p_perm_slice, arv_s, ind_s_perm_slice, lat_range_extend, lon_range_extend, depth_range, dx_depth, hull, ftrns1, ftrns2)\n",
    "    \n",
    "    if np.isnan(xmle).sum() > 0:\n",
    "        srcs_trv.append(np.nan*np.ones((1, 4)))\n",
    "        srcs_sigma.append(np.nan)\n",
    "        del_arv_p.append(0)\n",
    "        del_arv_s.append(0)\n",
    "        continue\n",
    "\n",
    "    pred_out = trv(torch.Tensor(locs_use_slice).to(device), torch.Tensor(xmle).to(device)).cpu().detach().numpy() + srcs_refined[i,3]\n",
    "\n",
    "    res_p = pred_out[0,ind_p_perm_slice,0] - arv_p\n",
    "    res_s = pred_out[0,ind_s_perm_slice,1] - arv_s\n",
    "\n",
    "    mean_shift = 0.0\n",
    "    cnt_phases = 0\n",
    "    if len(res_p) > 0:\n",
    "        mean_shift += np.median(res_p)*(len(res_p)/(len(res_p) + len(res_s)))\n",
    "        cnt_phases += 1\n",
    "\n",
    "    if len(res_s) > 0:\n",
    "        mean_shift += np.median(res_s)*(len(res_s)/(len(res_p) + len(res_s)))\n",
    "        cnt_phases += 1\n",
    "\n",
    "    ## This moved later, after the quality check\n",
    "    # srcs_trv.append(np.concatenate((xmle, np.array([srcs_refined[i,3] - mean_shift]).reshape(1,-1)), axis = 1))\n",
    "\n",
    "    ## Estimate uncertainties\n",
    "    origin = srcs_refined[i,3] - mean_shift\n",
    "    pred_out = trv(torch.Tensor(locs_use_slice).to(device), torch.Tensor(xmle[0,0:3].reshape(1,-1)).to(device)).cpu().detach().numpy() + origin # srcs_trv[-1][0,3]\n",
    "    res_p = pred_out[0,ind_p_perm_slice,0] - arv_p\n",
    "    res_s = pred_out[0,ind_s_perm_slice,1] - arv_s\n",
    "    \n",
    "    # use_quality_check = process_config['use_quality_check'] ## If True, check all associated picks and set a maximum allowed relative error after obtaining initial location\n",
    "    # max_relative_error = process_config['max_relative_error'] ## 0.15 corresponds to 15% maximum relative error allowed\n",
    "    # min_time_buffer = process_config['min_time_buffer'] ## Uses this time (seconds) as a minimum residual time, beneath which, the relative error criterion is ignored (i.e., an associated pick is removed if both the relative error > max_relative_error and the residual > min_time_buffer)\n",
    "    if use_quality_check == True:\n",
    "        tval_p = pred_out[0,ind_p_perm_slice,0] - origin\n",
    "        tval_s = pred_out[0,ind_s_perm_slice,1] - origin\n",
    "        tval_p[tval_p <= 0] = 0.01\n",
    "        tval_s[tval_s <= 0] = 0.01\n",
    "        rel_error_p = np.abs(res_p/tval_p)\n",
    "        rel_error_s = np.abs(res_s/tval_s)\n",
    "        # idel_p = np.where((rel_error_p > max_relative_error)*((pred_out[0,ind_p_perm_slice,0] - origin) > min_time_buffer))[0]\n",
    "        # idel_s = np.where((rel_error_s > max_relative_error)*((pred_out[0,ind_s_perm_slice,1] - origin) > min_time_buffer))[0]\n",
    "        idel_p = np.where((rel_error_p > max_relative_error)*(np.abs(res_p) > min_time_buffer))[0]\n",
    "        idel_s = np.where((rel_error_s > max_relative_error)*(np.abs(res_s) > min_time_buffer))[0]\n",
    "        del_arv_p.append(len(idel_p))\n",
    "        del_arv_s.append(len(idel_s))\n",
    "                  \n",
    "        if len(idel_p) > 0:\n",
    "            arv_p = np.delete(arv_p, idel_p, axis = 0)\n",
    "            ind_p = np.delete(ind_p, idel_p, axis = 0)\n",
    "            Picks_P[i] = np.delete(Picks_P[i], idel_p, axis = 0)\n",
    "            Picks_P_perm[i] = np.delete(Picks_P_perm[i], idel_p, axis = 0)\n",
    "\n",
    "        if len(idel_s) > 0:\n",
    "            arv_s = np.delete(arv_s, idel_s, axis = 0)\n",
    "            ind_s = np.delete(ind_s, idel_s, axis = 0)\n",
    "            Picks_S[i] = np.delete(Picks_S[i], idel_s, axis = 0)\n",
    "            Picks_S_perm[i] = np.delete(Picks_S_perm[i], idel_s, axis = 0)\n",
    "        \n",
    "        ind_unique_arrivals = np.sort(np.unique(np.concatenate((ind_p, ind_s), axis = 0)).astype('int'))\n",
    "\n",
    "        if len(ind_unique_arrivals) == 0:\n",
    "            srcs_trv.append(np.nan*np.ones((1, 4)))\n",
    "            srcs_sigma.append(np.nan)\n",
    "            continue\t\t\t\n",
    "        \n",
    "        perm_vec_arrivals = -1*np.ones(locs_use.shape[0]).astype('int')\n",
    "        perm_vec_arrivals[ind_unique_arrivals] = np.arange(len(ind_unique_arrivals))\n",
    "        locs_use_slice = locs_use[ind_unique_arrivals]\n",
    "        ind_p_perm_slice = perm_vec_arrivals[ind_p]\n",
    "        ind_s_perm_slice = perm_vec_arrivals[ind_s]\n",
    "        \n",
    "        if len(ind_p_perm_slice) > 0:\n",
    "            assert(ind_p_perm_slice.min() > -1)\n",
    "        if len(ind_s_perm_slice) > 0:\n",
    "            assert(ind_s_perm_slice.min() > -1)\n",
    "            \n",
    "        # if len(ind_unique_arrivals) == 0:\n",
    "        # \tsrcs_trv.append(np.nan*np.ones((1, 4)))\n",
    "        # \tcontinue\t\t\t\t\n",
    "                            \n",
    "        if ((len(idel_p) > 0) + (len(idel_s) > 0)) > 0: ## If arrivals have been removed, re-locate\n",
    "\n",
    "            if (min_required_picks is not False)*(min_required_sta is not False):\n",
    "                \n",
    "                if ((len(ind_unique_arrivals) == 0) + ((len(arv_p) + len(arv_s)) < min_required_picks) + (len(np.unique(np.concatenate((ind_p, ind_s), axis = 0))) < min_required_sta)) > 0:\n",
    "                    srcs_trv.append(np.nan*np.ones((1, 4)))\n",
    "                    srcs_sigma.append(np.nan)\n",
    "                    continue\n",
    "\n",
    "            else:\n",
    "\n",
    "                if len(ind_unique_arrivals) == 0:\n",
    "                    srcs_trv.append(np.nan*np.ones((1, 4)))\n",
    "                    srcs_sigma.append(np.nan)\n",
    "                    continue\n",
    "            \n",
    "            if use_differential_evolution_location == True:\n",
    "                xmle, logprob = differential_evolution_location(trv, locs_use_slice, arv_p, ind_p_perm_slice, arv_s, ind_s_perm_slice, lat_range_extend, lon_range_extend, depth_range, surface_profile = surface_profile, device = device)\n",
    "            else:\n",
    "                xmle, logprob, Swarm = MLE_particle_swarm_location_one_mean_stable_depth_with_hull(trv, locs_use_slice, arv_p, ind_p_perm_slice, arv_s, ind_s_perm_slice, lat_range_extend, lon_range_extend, depth_range, dx_depth, hull, ftrns1, ftrns2)\n",
    "            \n",
    "        if np.isnan(xmle).sum() > 0:\n",
    "            srcs_trv.append(np.nan*np.ones((1, 4)))\n",
    "            srcs_sigma.append(np.nan)\n",
    "            continue\n",
    "            \n",
    "        pred_out = trv(torch.Tensor(locs_use_slice).to(device), torch.Tensor(xmle[0,0:3].reshape(1,-1)).to(device)).cpu().detach().numpy() + srcs_refined[i,3] # srcs_trv[-1][0,3]\n",
    "        res_p = pred_out[0,ind_p_perm_slice,0] - arv_p\n",
    "        res_s = pred_out[0,ind_s_perm_slice,1] - arv_s\n",
    "        \n",
    "        mean_shift = 0.0\n",
    "        cnt_phases = 0\n",
    "        if len(res_p) > 0:\n",
    "            mean_shift += np.median(res_p)*(len(res_p)/(len(res_p) + len(res_s)))\n",
    "            cnt_phases += 1\n",
    "\n",
    "        if len(res_s) > 0:\n",
    "            mean_shift += np.median(res_s)*(len(res_s)/(len(res_p) + len(res_s)))\n",
    "            cnt_phases += 1\t\t\n",
    "    else:\n",
    "        del_arv_p.append(0)\n",
    "        del_arv_s.append(0)\n",
    "\n",
    "    origin = srcs_refined[i,3] - mean_shift\n",
    "    pred_out = trv(torch.Tensor(locs_use_slice).to(device), torch.Tensor(xmle[0,0:3].reshape(1,-1)).to(device)).cpu().detach().numpy() + origin # srcs_trv[-1][0,3]\n",
    "    res_p = pred_out[0,ind_p_perm_slice,0] - arv_p\n",
    "    res_s = pred_out[0,ind_s_perm_slice,1] - arv_s\n",
    "    \n",
    "    scale_val1 = 100.0*np.linalg.norm(ftrns1(xmle[0,0:3].reshape(1,-1)) - ftrns1(xmle[0,0:3].reshape(1,-1) + np.array([0.01, 0, 0]).reshape(1,-1)), axis = 1)[0]\n",
    "    scale_val2 = 100.0*np.linalg.norm(ftrns1(xmle[0,0:3].reshape(1,-1)) - ftrns1(xmle[0,0:3].reshape(1,-1) + np.array([0.0, 0.01, 0]).reshape(1,-1)), axis = 1)[0]\n",
    "    scale_val = 0.5*(scale_val1 + scale_val2)\n",
    "\n",
    "    scale_partials = (1/60.0)*np.array([1.0, 1.0, scale_val]).reshape(1,-1)\n",
    "    src_input_p = Variable(torch.Tensor(xmle[0,0:3].reshape(1,-1)).repeat(len(ind_p_perm_slice),1).to(device), requires_grad = True)\n",
    "    src_input_s = Variable(torch.Tensor(xmle[0,0:3].reshape(1,-1)).repeat(len(ind_s_perm_slice),1).to(device), requires_grad = True)\n",
    "    trv_out_p = trv_pairwise1(torch.Tensor(locs_use_slice[ind_p_perm_slice]).to(device), src_input_p, method = 'direct')[:,0]\n",
    "    trv_out_s = trv_pairwise1(torch.Tensor(locs_use_slice[ind_s_perm_slice]).to(device), src_input_s, method = 'direct')[:,1]\n",
    "    # trv_out = trv_out[np.arange(len(trv_out)), arrivals[n_inds_picks[i],4].astype('int')] # .cpu().detach().numpy() ## Select phase type\n",
    "    d_p = scale_partials*torch.autograd.grad(inputs = src_input_p, outputs = trv_out_p, grad_outputs = torch.ones(len(trv_out_p)).to(device), retain_graph = True, create_graph = True, allow_unused = True)[0].cpu().detach().numpy()\n",
    "    d_s = scale_partials*torch.autograd.grad(inputs = src_input_s, outputs = trv_out_s, grad_outputs = torch.ones(len(trv_out_s)).to(device), retain_graph = True, create_graph = True, allow_unused = True)[0].cpu().detach().numpy()\n",
    "    \n",
    "    d_grad = np.concatenate((d_p, d_s), axis = 0)\n",
    "    sig_d = 0.15 ## Assumed pick uncertainty (seconds)\n",
    "    chi_pdf = chi2(df = 3).pdf(0.99)\n",
    "    \n",
    "    var = (d_grad/scale_partials)\n",
    "    var = np.linalg.pinv(var.T@var)*(sig_d**2)\n",
    "    var = var*chi_pdf\n",
    "    #Variances.append(np.expand_dims(var, axis = 0))\n",
    "    var_cart = (d_grad/scale_partials)/np.array([scale_val1, scale_val2, 1.0]).reshape(1,-1)\n",
    "    var_cart = np.linalg.pinv(var_cart.T@var_cart)*(sig_d**2)\n",
    "    var_cart = var_cart*chi_pdf\n",
    "    sigma_cart = np.linalg.norm(np.diag(var_cart)**(0.5))\n",
    "\n",
    "    ## Append the final location and origin time\n",
    "    srcs_trv.append(np.concatenate((xmle, np.array([origin]).reshape(1,-1)), axis = 1))\n",
    "    srcs_sigma.append(sigma_cart)\n",
    "\n",
    "\n",
    "srcs_trv = np.vstack(srcs_trv)\n",
    "srcs_sigma = np.hstack(srcs_sigma)\n",
    "del_arv_p = np.hstack(del_arv_p)\n",
    "del_arv_s = np.hstack(del_arv_s)\n",
    "assert(len(srcs_trv) == len(srcs_sigma))\n",
    "assert(len(srcs_trv) == len(del_arv_p))\n",
    "assert(len(srcs_trv) == len(del_arv_s))\n",
    "assert(len(srcs_trv) == len(Picks_P))\n",
    "assert(len(srcs_trv) == len(Picks_S))\n",
    "assert(len(srcs_trv) == len(srcs_refined))\n",
    "###### Only keep events with minimum number of picks and observing stations #########\n",
    "\n",
    "# Count number of P and S picks\n",
    "cnt_p, cnt_s = np.zeros(srcs_refined.shape[0]), np.zeros(srcs_refined.shape[0])\n",
    "for i in range(srcs_refined.shape[0]):\n",
    "    cnt_p[i] = Picks_P[i].shape[0]\n",
    "    cnt_s[i] = Picks_S[i].shape[0]\n",
    "\n",
    "# min_required_picks = 6\n",
    "# min_required_sta = 6\n",
    "\n",
    "if (min_required_picks is not False)*(min_required_sta is not False):\n",
    "\n",
    "    ikeep_picks = np.where((cnt_p + cnt_s) >= min_required_picks)[0]\n",
    "    ikeep_sta = np.where(np.array([len(np.unique(np.concatenate((Picks_P[j][:,1], Picks_S[j][:,1]), axis = 0))) for j in range(len(srcs_refined))]) >= min_required_sta)[0]\n",
    "    ikeep = np.sort(np.array(list(set(ikeep_picks).intersection(ikeep_sta))))\n",
    "\n",
    "    srcs_refined = srcs_refined[ikeep]\n",
    "    srcs_trv = srcs_trv[ikeep]\n",
    "    srcs_sigma = srcs_sigma[ikeep]\n",
    "    del_arv_p = del_arv_p[ikeep]\n",
    "    del_arv_s = del_arv_s[ikeep]\n",
    "    cnt_p = cnt_p[ikeep]\n",
    "    cnt_s = cnt_s[ikeep]\n",
    "\n",
    "    if len(srcs_trv) == 0:\n",
    "        print('No events left after minimum pick requirements')\n",
    "        # continue\n",
    "\n",
    "    Picks_P = [Picks_P[j] for j in ikeep]\n",
    "    Picks_S = [Picks_S[j] for j in ikeep]\n",
    "\n",
    "    Picks_P_perm = [Picks_P_perm[j] for j in ikeep]\n",
    "    Picks_S_perm = [Picks_S_perm[j] for j in ikeep]\n",
    "\n",
    "print('Detected %d events'%len(srcs_trv))\n",
    "\n",
    "####################################################################################\n",
    "\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa42b94-4b94-4b9b-96af-b0bc4032da1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot travel time located sources\n",
    "fig, ax = plt.subplots(figsize = [8,8])\n",
    "plt.scatter(locs[:,1], locs[:,0], c = 'r', marker = '^')\n",
    "plt.scatter(srcs_trv[:,1], srcs_trv[:,0], c = 'C1', s = 6.5, marker = 'o', label = 'Ours')\n",
    "# plt.scatter(srcs_known[:,1], srcs_known[:,0], c = 'C0', s = 4, marker = 'o', alpha = 0.8, label = 'USGS')\n",
    "ax.set_aspect(1.0/np.cos(np.pi*locs[:,0].mean()/180.0)) ## Set projection aspect ratio\n",
    "d_pad = 0.5 ## Pad 0.5 degrees to plot axes\n",
    "ax.set_xlim(np.array(ax.get_xlim()) + np.array([-d_pad, d_pad]))\n",
    "ax.set_ylim(np.array(ax.get_ylim()) + np.array([-d_pad, d_pad]))\n",
    "plt.legend()\n",
    "plt.show(block = False)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize = [8,8])\n",
    "plt.scatter(locs[:,1], locs[:,0], c = 'r', marker = '^')\n",
    "plt.scatter(srcs_trv[:,1], srcs_trv[:,0], c = 'C1', s = 6.5, marker = 'o', label = 'Ours')\n",
    "plt.scatter(srcs_known[:,1], srcs_known[:,0], c = 'C0', s = 3.0, marker = 'o', alpha = 0.65, label = 'USGS')\n",
    "ax.set_aspect(1.0/np.cos(np.pi*locs[:,0].mean()/180.0)) ## Set projection aspect ratio\n",
    "d_pad = 0.5 ## Pad 0.5 degrees to plot axes\n",
    "ax.set_xlim(np.array(ax.get_xlim()) + np.array([-d_pad, d_pad]))\n",
    "ax.set_ylim(np.array(ax.get_ylim()) + np.array([-d_pad, d_pad]))\n",
    "plt.legend()\n",
    "plt.show(block = False)\n",
    "print('How is quality? Is it improved? In what way?')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456649d6-1998-40f7-a36a-bb762aa03542",
   "metadata": {},
   "source": [
    "# Let's now find \"matched\" earthquakes to the USGS events\n",
    "<pre>\n",
    "    (i). (i.e., events within a nearby spatial/temporal window)\n",
    "    \n",
    "<pre>\n",
    "    (ii). Can use the \"maximum_bipartite_assignment\" function in GENIE.\n",
    "    \n",
    "<pre>\n",
    "    (iii). Note: deteremining \"matched\" events is somewhat non-trivial (requires optimization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6492b5-bbb1-485f-856a-8fff456bcad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "temporal_win_match = 8.0 ## 10 s window match threshold\n",
    "spatial_win_match = 30e3 ## 30 km match threshold\n",
    "\n",
    "## First select only subset of \"known\" events within the time window we're processing (since we clipped the window to part of the day\n",
    "ifind = np.where((srcs_known[:,3] > clip_win_start)*(srcs_known[:,3] < (clip_win_start + clip_win_duration)))[0]\n",
    "srcs_known = srcs_known[ifind]\n",
    "ifind_not_nan = np.where(np.isnan(srcs_trv[:,0]) == 0)[0]\n",
    "\n",
    "matches1 = maximize_bipartite_assignment(srcs_known, srcs_refined, ftrns1, ftrns2, temporal_win = temporal_win_match, spatial_win = spatial_win_match, verbose = False)[0]\n",
    "if len(ifind_not_nan) > 0:\n",
    "    matches2 = maximize_bipartite_assignment(srcs_known, srcs_trv[ifind_not_nan], ftrns1, ftrns2, temporal_win = temporal_win_match, spatial_win = spatial_win_match, verbose = False)[0]\n",
    "    matches2[:,1] = ifind_not_nan[matches2[:,1]]\n",
    "else:\n",
    "    matches2 = np.nan*np.zeros((0,2))\n",
    "## matches1 and matches2 record the matched indices between USGS and detected initial events and travel time location events)\n",
    "\n",
    "print('Matched %d events of %d USGS events'%(len(matches2), len(srcs_known)))\n",
    "print('and detected %d additional new events (over %0.2f hr time interval)'%(len(srcs_trv) - len(srcs_known), clip_win_duration/3600.0))\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b00b60-4d51-4451-b279-bad25d1fb7af",
   "metadata": {},
   "source": [
    "# We'll now plot the matched earthquake origin time and spatial residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97695783-a2c2-4dbc-91d0-665dd0156608",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2,2, figsize = [10,10])\n",
    "res = srcs_trv[matches2[:,1],0:4] - srcs_known[matches2[:,0],0:4]\n",
    "ax[0,0].hist(res[:,0], 15, linewidth = 1.0, alpha = 0.7, edgecolor = 'black')\n",
    "ax[0,1].hist(res[:,1], 15, linewidth = 1.0, alpha = 0.7, edgecolor = 'black')\n",
    "ax[1,0].hist(res[:,2]/1000.0, 15, linewidth = 1.0, alpha = 0.7, edgecolor = 'black')\n",
    "ax[1,1].hist(res[:,3], 15, linewidth = 1.0, alpha = 0.7, edgecolor = 'black')\n",
    "ax[0,0].set_xlabel('Latitude')\n",
    "ax[0,1].set_xlabel('Longitude')\n",
    "ax[1,0].set_xlabel('Depth (km)')\n",
    "ax[1,1].set_xlabel('Origin Time (s)')\n",
    "\n",
    "\n",
    "## Map view comparison\n",
    "fig, ax = plt.subplots(figsize = [8,8])\n",
    "ax.scatter(locs[:,1], locs[:,0], c = 'r', marker = '^')\n",
    "ax.scatter(srcs_trv[:,1], srcs_trv[:,0], c = 'C1', s = 6.5, marker = 'o', label = 'Ours')\n",
    "ax.scatter(srcs_known[:,1], srcs_known[:,0], c = 'C0', s = 3.0, marker = 'o', alpha = 0.65, label = 'USGS')\n",
    "for j in range(len(matches2)):\n",
    "    ax.plot([srcs_trv[matches2[j,1],1], srcs_known[matches2[j,0],1]], [srcs_trv[matches2[j,1],0], srcs_known[matches2[j,0],0]], c = 'black', alpha = 0.8, linewidth = 0.5)\n",
    "# plt.plot(np.concatenate((srcs_trv[matches2[:,1],1].reshape(1,-1), srcs_known[matches2[:,0],1].reshape(1,-1)), axis = 1).T, np.concatenate((srcs_trv[matches2[:,1],0].reshape(1,-1), srcs_known[matches2[:,0],0].reshape(1,-1)), axis = 1).T, c = 'black', alpha = 0.2, linewidth = 0.5)\n",
    "ax.set_aspect(1.0/np.cos(np.pi*locs[:,0].mean()/180.0)) ## Set projection aspect ratio\n",
    "# d_pad = 0.5 ## Pad 0.5 degrees to plot axes\n",
    "ax.set_xlim(np.array(ax.get_xlim()) + np.array([-d_pad*0.2, d_pad*0.2]))\n",
    "ax.set_ylim(np.array(ax.get_ylim()) + np.array([-d_pad*0.2, d_pad*0.2]))\n",
    "plt.legend()\n",
    "plt.show(block = False)\n",
    "\n",
    "\n",
    "## Depth view comparison\n",
    "fig, ax = plt.subplots(figsize = [8,8])\n",
    "ax.scatter(locs[:,1], locs[:,2], c = 'r', marker = '^')\n",
    "ax.scatter(srcs_trv[:,1], srcs_trv[:,2], c = 'C1', s = 6.5, marker = 'o', label = 'Ours')\n",
    "ax.scatter(srcs_known[:,1], srcs_known[:,2], c = 'C0', s = 3.0, marker = 'o', alpha = 0.65, label = 'USGS')\n",
    "for j in range(len(matches2)):\n",
    "    ax.plot([srcs_trv[matches2[j,1],1], srcs_known[matches2[j,0],1]], [srcs_trv[matches2[j,1],2], srcs_known[matches2[j,0],2]], c = 'black', alpha = 0.8, linewidth = 0.5)\n",
    "# plt.plot(np.concatenate((srcs_trv[matches2[:,1],1].reshape(1,-1), srcs_known[matches2[:,0],1].reshape(1,-1)), axis = 1).T, np.concatenate((srcs_trv[matches2[:,1],0].reshape(1,-1), srcs_known[matches2[:,0],0].reshape(1,-1)), axis = 1).T, c = 'black', alpha = 0.2, linewidth = 0.5)\n",
    "# ax.set_aspect(1.0/np.cos(np.pi*locs[:,0].mean()/180.0)) ## Set projection aspect ratio\n",
    "# d_pad = 0.5 ## Pad 0.5 degrees to plot axes\n",
    "# ax.set_xlim(np.array(ax.get_xlim()) + np.array([-d_pad*0.2, d_pad*0.2]))\n",
    "# ax.set_ylim(np.array(ax.get_ylim()) + np.array([-d_pad*0.2, d_pad*0.2]))\n",
    "plt.legend()\n",
    "plt.show(block = False)\n",
    "\n",
    "# r = srcs_trv[matches2[:,1],0:4] - srcs_known[matches2[:,0],0:4]\n",
    "# print(r.max(0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5b4e1b-9b73-42d5-a90e-386210cd76bd",
   "metadata": {},
   "source": [
    "# Compute magnitudes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a8b46d-3f05-4444-aa04-2422b267e1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's calibrate a magnitude scale between our events and matched USGS events\n",
    "# Use relationship log_amp = C1*Mag - C2*log_dist + C_i\n",
    "\n",
    "Matches = np.copy(matches2)\n",
    "srcs_w = srcs[:,4]\n",
    "min_threshold = 0.0\n",
    "srcs_ref = np.copy(srcs_known)\n",
    "\n",
    "## Setup probability of sampling different matched events\n",
    "prob = np.ones(len(Matches))\n",
    "for i in range(len(Matches)):\n",
    "\tif (cnt_p[Matches[i,1]] == 0) or (cnt_s[Matches[i,1]] == 0): ## Skip events missing all of p or s (otherwise might have an indexing issue)\n",
    "\t\tprob[i] = 0.0\n",
    "\tif srcs_w[Matches[i,1]] < min_threshold:\n",
    "\t\tprob[i] = 0.0\n",
    "\tif (cnt_p[Matches[i,1]] + cnt_s[Matches[i,1]]) < min_picks:\n",
    "\t\tprob[i] = 0.0\n",
    "\n",
    "## Fit magnitude model\n",
    "\n",
    "prob = prob/prob.sum()\n",
    "print('Retained %0.8f of matches'%(len(np.where(prob > 0)[0])/len(Matches)))\n",
    "\n",
    "# ## Setup spatial graph and create laplacian\n",
    "# k_spc_edges = 25 # 50 ## smooth the spatial coefficients\n",
    "# x_grid = x_grids[0]\n",
    "# A_src_src = knn(torch.Tensor(ftrns1(x_grid)/1000.0), torch.Tensor(ftrns1(x_grid)/1000.0), k = k_spc_edges + 1).flip(0).long().contiguous().to(device) # )[0]\n",
    "# lap = get_laplacian(A_src_src, normalization = 'rw')\n",
    "\n",
    "# ## Initilize Laplace classes\n",
    "# k_interp = 15\n",
    "# Lap = Laplacian(lap[0], lap[1])\n",
    "# # Interp = InterpolateAnisotropicStations(k = k_interp, device = device)\n",
    "\n",
    "use_scalar_station_corrections = True\n",
    "if use_scalar_station_corrections == True:\n",
    "\tmag_grid = locs.mean(0).reshape(1,-1)\n",
    "\tk_grid = 1\n",
    "else:\n",
    "\tn_mag_grid = 30\n",
    "\tmag_grid = kmeans_packing_sampling_points(scale_x_extend, offset_x_extend, 3, n_mag_grid, ftrns1, n_batch = 3000, n_steps = 3000, n_sim = 1)[0]\n",
    "\tk_grid = 5\n",
    "\n",
    "Mag = Magnitude(torch.Tensor(locs).to(device), torch.Tensor(mag_grid).to(device), ftrns1_diff, ftrns2_diff, k = k_grid, device = device).to(device)\n",
    "\n",
    "optimizer = optim.Adam(Mag.parameters(), lr = 0.01)\n",
    "schedular = StepLR(optimizer, 1000, gamma = 0.8)\n",
    "loss_func = nn.MSELoss()\n",
    "\n",
    "\n",
    "n_updates = 1500\n",
    "n_batch = 100\n",
    "use_difference_loss = True\n",
    "iuse_p = np.where([len(Picks_P[i]) >= 2 for i in range(len(Picks_P))])[0]\n",
    "iuse_s = np.where([len(Picks_S[i]) >= 2 for i in range(len(Picks_S))])[0]\n",
    "\n",
    "## Applying fitting\n",
    "losses = []\n",
    "for i in range(n_updates):\n",
    "\n",
    "\toptimizer.zero_grad()\n",
    "\n",
    "\ti0 = np.random.choice(len(Matches), size = n_batch, p = prob)\n",
    "\n",
    "\tref_ind, srcs_ind = Matches[i0,0], Matches[i0,1]\n",
    "\n",
    "\tarv_p = torch.Tensor(np.hstack([Picks_P[j][:,0].astype('int') for j in srcs_ind])).to(device)\n",
    "\tarv_s = torch.Tensor(np.hstack([Picks_S[j][:,0].astype('int') for j in srcs_ind])).to(device)\n",
    "\tamp_p = torch.Tensor(np.hstack([Picks_P[j][:,2] for j in srcs_ind])).to(device)\n",
    "\tamp_s = torch.Tensor(np.hstack([Picks_S[j][:,2] for j in srcs_ind])).to(device)\n",
    "\tnum_p = np.array([len(Picks_P[j]) for j in srcs_ind])\n",
    "\tnum_s = np.array([len(Picks_S[j]) for j in srcs_ind])\n",
    "\tind_p = np.hstack([Picks_P[j][:,1].astype('int') for j in srcs_ind])\n",
    "\tind_s = np.hstack([Picks_S[j][:,1].astype('int') for j in srcs_ind])\n",
    "\n",
    "\tcat_slice_single = torch.Tensor(np.concatenate((srcs[srcs_ind,0:3], srcs_ref[ref_ind,4].reshape(-1,1)), axis = 1)).to(device) # .repeat_interleave(torch.Tensor(num_p).to(device).long(), dim = 0)\n",
    "\n",
    "\tcat_slice_p = torch.Tensor(cat_slice_single).to(device).repeat_interleave(torch.Tensor(num_p).to(device).long(), dim = 0)\n",
    "\tcat_slice_s = torch.Tensor(cat_slice_single).to(device).repeat_interleave(torch.Tensor(num_s).to(device).long(), dim = 0)\n",
    "\n",
    "\n",
    "\tlog_amp_p = Mag.train(torch.Tensor(ind_p).long().to(device), cat_slice_p[:,0:3], cat_slice_p[:,3], torch.zeros(len(ind_p)).long().to(device))\n",
    "\tlog_amp_s = Mag.train(torch.Tensor(ind_s).long().to(device), cat_slice_s[:,0:3], cat_slice_s[:,3], torch.ones(len(ind_s)).long().to(device))\n",
    "\n",
    "\tloss1 = loss_func(torch.log10(amp_p), log_amp_p)\n",
    "\tloss2 = loss_func(torch.log10(amp_s), log_amp_s)\n",
    "\tloss = 0.5*loss1 + 0.5*loss2\n",
    "\n",
    "\tif use_difference_loss == True: ## If True, also compute pairwise log_amplitude differences (for different stations, and fixed sources), since\n",
    "\t\t## these cancel out the effect of the magnitude; and hence, this provides an unsupervised target to constrain the amplitude-distance\n",
    "\t\t## attenuation relationships (i.e., Trugman, 2024; SRL: A HighPrecision Earthquake Catalog for Nevada).\n",
    "\t\tichoose_p = np.random.choice(iuse_p, size = int(n_batch/2))\n",
    "\t\tichoose_s = np.random.choice(iuse_s, size = int(n_batch/2))\n",
    "\t\tichoose_p1 = [np.random.choice(len(Picks_P[ichoose_p[j]]), size = 2, replace = False) for j in range(len(ichoose_p))]\n",
    "\t\tichoose_s1 = [np.random.choice(len(Picks_S[ichoose_s[j]]), size = 2, replace = False) for j in range(len(ichoose_s))]\n",
    "\t\t\n",
    "\t\tind_p1, ind_p2 = np.array([Picks_P[ichoose_p[j]][ichoose_p1[j][0],1] for j in range(len(ichoose_p))]), np.array([Picks_P[ichoose_p[j]][ichoose_p1[j][1],1] for j in range(len(ichoose_p))])\n",
    "\t\tamp_p1, amp_p2 = np.array([Picks_P[ichoose_p[j]][ichoose_p1[j][0],2] for j in range(len(ichoose_p))]), np.array([Picks_P[ichoose_p[j]][ichoose_p1[j][1],2] for j in range(len(ichoose_p))])\n",
    "\n",
    "\t\tind_s1, ind_s2 = np.array([Picks_S[ichoose_s[j]][ichoose_s1[j][0],1] for j in range(len(ichoose_s))]), np.array([Picks_S[ichoose_s[j]][ichoose_s1[j][1],1] for j in range(len(ichoose_s))])\n",
    "\t\tamp_s1, amp_s2 = np.array([Picks_S[ichoose_s[j]][ichoose_s1[j][0],2] for j in range(len(ichoose_s))]), np.array([Picks_S[ichoose_s[j]][ichoose_s1[j][1],2] for j in range(len(ichoose_s))])\n",
    "\n",
    "\t\t## Differential P amplitude loss\n",
    "\t\tlog_amp_p1 = Mag.train(torch.Tensor(ind_p1).long().to(device), torch.Tensor(srcs[ichoose_p,0:3]).to(device), torch.ones(len(ichoose_p)).to(device), torch.zeros(len(ind_p1)).long().to(device)) ## Note: effect of magnitude  will be canceled out\n",
    "\t\tlog_amp_p2 = Mag.train(torch.Tensor(ind_p2).long().to(device), torch.Tensor(srcs[ichoose_p,0:3]).to(device), torch.ones(len(ichoose_p)).to(device), torch.zeros(len(ind_p2)).long().to(device)) ## Note: effect of magnitude  will be canceled out\n",
    "\t\tlog_amp_p_diff = log_amp_p1 - log_amp_p2\n",
    "\t\ttrgt_amp_p_diff = torch.Tensor(np.log10(amp_p1) - np.log10(amp_p2)).to(device)\n",
    "\t\tloss_diff_p = loss_func(log_amp_p_diff, trgt_amp_p_diff)\n",
    "\n",
    "\t\t## Differential S amplitude loss\n",
    "\t\tlog_amp_s1 = Mag.train(torch.Tensor(ind_s1).long().to(device), torch.Tensor(srcs[ichoose_s,0:3]).to(device), torch.ones(len(ichoose_s)).to(device), torch.ones(len(ind_s1)).long().to(device)) ## Note: effect of magnitude  will be canceled out\n",
    "\t\tlog_amp_s2 = Mag.train(torch.Tensor(ind_s2).long().to(device), torch.Tensor(srcs[ichoose_s,0:3]).to(device), torch.ones(len(ichoose_s)).to(device), torch.ones(len(ind_s2)).long().to(device)) ## Note: effect of magnitude  will be canceled out\n",
    "\t\tlog_amp_s_diff = log_amp_s1 - log_amp_s2\n",
    "\t\ttrgt_amp_s_diff = torch.Tensor(np.log10(amp_s1) - np.log10(amp_s2)).to(device)\n",
    "\t\tloss_diff_s = loss_func(log_amp_s_diff, trgt_amp_s_diff)\n",
    "\n",
    "\t\tloss_diff = 0.5*loss_diff_p + 0.5*loss_diff_s\n",
    "\n",
    "\t\t## Take the mean loss\n",
    "\t\tloss = 0.5*loss + 0.5*loss_diff\n",
    "\n",
    "\n",
    "\tloss.backward()\n",
    "\toptimizer.step()\n",
    "\tschedular.step()\n",
    "\tlosses.append(loss.item())\n",
    "    # if np.mod(i, 5) == 0:\n",
    "    # print('%d %0.5f'%(i, loss.item()))\n",
    "\n",
    "\tassert(torch.abs(log_amp_p).max().item() < 100)\n",
    "\tassert(torch.abs(log_amp_s).max().item() < 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26e3a2f-a059-41e9-b333-2a0da6c65a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute magnitudes\n",
    "Mag_pred = []\n",
    "for i in range(len(srcs_trv)):\n",
    "\n",
    "    ind_p, log_amp_p = Picks_P[i][:,1].astype('int'), np.log10(Picks_P[i][:,2])\n",
    "    ind_s, log_amp_s = Picks_S[i][:,1].astype('int'), np.log10(Picks_S[i][:,2])\n",
    "\n",
    "    mag_p = Mag(torch.Tensor(ind_p).long().to(device), torch.Tensor(srcs[i,0:3].reshape(1,-1)).to(device), torch.Tensor(log_amp_p).to(device), torch.zeros(len(ind_p)).long().to(device))\n",
    "    mag_s = Mag(torch.Tensor(ind_s).long().to(device), torch.Tensor(srcs[i,0:3].reshape(1,-1)).to(device), torch.Tensor(log_amp_s).to(device), torch.ones(len(ind_s)).long().to(device))\n",
    "\n",
    "    mag_pred = np.median(np.concatenate((mag_p.cpu().detach().numpy().reshape(-1), mag_s.cpu().detach().numpy().reshape(-1)), axis = 0))\n",
    "    Mag_pred.append(mag_pred)\n",
    "    if np.mod(i, 50) == 0:\n",
    "        print(i)\n",
    "\n",
    "mag_pred = np.hstack(Mag_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41153c7e-fa6e-477a-a8a2-9c4e934723c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot magnitudes\n",
    "fig, ax = plt.subplots(figsize = [10,8])\n",
    "ax.scatter(srcs_trv[:,3], mag_pred, c = 'C0', label = 'Ours')\n",
    "ax.scatter(srcs_known[:,3], srcs_known[:,4], c = 'C1', label = 'USGS')\n",
    "ax.set_xlabel('Time (s)')\n",
    "ax.set_ylabel('Magnitude')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4661b9-c6a2-4a9d-bce0-b4ddf88ead78",
   "metadata": {},
   "source": [
    "# Write catalog file to HypoDD format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1cf030-4754-49e4-98b2-8d73b302b840",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/home/jovyan/')\n",
    "\n",
    "mags = np.copy(mag_pred)\n",
    "\n",
    "min_assoc_val = min([np.array([Picks_P[j][:,-1].min(), Picks_S[j][:,-1].min()]).min() for j in range(len(Picks_P))])\n",
    "max_assoc_val = max([np.array([Picks_P[j][:,-1].max(), Picks_S[j][:,-1].max()]).max() for j in range(len(Picks_P))])\n",
    "\n",
    "max_assoc_val = max([1.0, max_assoc_val])\n",
    "pval = np.polyfit([min_assoc_val, max_assoc_val], [0.5, 1.0], 1)\n",
    "pmap = lambda x: np.polyval(pval, x)\n",
    "\n",
    "trv_out1 = trv(torch.Tensor(locs_use).to(device), torch.Tensor(srcs_refined[:,0:3]).to(device)).cpu().detach().numpy() + srcs_refined[:,3].reshape(-1,1,1)\n",
    "trv_out1_all = trv(torch.Tensor(locs).to(device), torch.Tensor(srcs_refined[:,0:3]).to(device)).cpu().detach().numpy() + srcs_refined[:,3].reshape(-1,1,1) \n",
    "# trv_out2 = trv(torch.Tensor(locs_use), torch.Tensor(srcs_trv[:,0:3])).cpu().detach().numpy() + srcs_trv[:,3].reshape(-1,1,1) \n",
    "\n",
    "trv_out2 = np.nan*np.zeros((srcs_trv.shape[0], locs_use.shape[0], 2))\n",
    "trv_out2_all = np.nan*np.zeros((srcs_trv.shape[0], locs.shape[0], 2))\n",
    "ifind_not_nan = np.where(np.isnan(srcs_trv[:,0]) == 0)[0]\n",
    "if len(ifind_not_nan) > 0:\n",
    "    trv_out2[ifind_not_nan,:,:] = trv(torch.Tensor(locs_use).to(device), torch.Tensor(srcs_trv[ifind_not_nan,0:3]).to(device)).cpu().detach().numpy() + srcs_trv[ifind_not_nan,3].reshape(-1,1,1)\n",
    "    trv_out2_all[ifind_not_nan,:,:] = trv(torch.Tensor(locs).to(device), torch.Tensor(srcs_trv[ifind_not_nan,0:3]).to(device)).cpu().detach().numpy() + srcs_trv[ifind_not_nan,3].reshape(-1,1,1)\n",
    "    \n",
    "\n",
    "res_p = [trv_out2[j,Picks_P[j][:,1].astype('int'),0] - Picks_P[j][:,0] for j in range(len(srcs_trv))]\n",
    "res_s = [trv_out2[j,Picks_S[j][:,1].astype('int'),1] - Picks_S[j][:,0] for j in range(len(srcs_trv))]\n",
    "rms = np.array([np.linalg.norm(np.concatenate((res_p[j], res_s[j]), axis = 0))/np.sqrt(len(res_p[j]) + len(res_s[j])) for j in range(len(srcs_trv))])\n",
    "\n",
    "\n",
    "# ph2dt accepts hypocenter, followed by its travel time data in the following format:\n",
    "#, YR, MO, DY, HR, MN, SC, LAT, LON, DEP, MAG, EH, EZ, RMS, ID\n",
    "\n",
    "f = open(path_to_file + 'ph2dt_file_12_20_22.txt', 'w')\n",
    "for i in range(len(srcs_trv)):\n",
    "\n",
    "\tt0 = UTCDateTime(date[0], date[1], date[2]) + srcs_trv[i,3]\n",
    "\tsec_res = t0 - UTCDateTime(t0.year, t0.month, t0.day, t0.hour, t0.minute, t0.second)\n",
    "\n",
    "\tf.write('# %d %d %d %d %d %0.3f %0.4f %0.4f %0.3f %0.3f %0.3f %0.3f %0.3f %d \\n'%(t0.year, t0.month, t0.day, t0.hour, t0.minute, t0.second + sec_res, srcs_trv[i,0], srcs_trv[i,1], -1.0*srcs_trv[i,2]/1000.0, mags[i], uncertainity[i]/1000.0, uncertainity[i]/1000.0, rms[i], i + 1))\n",
    "\n",
    "\tfor j in range(len(Picks_P[i])):\n",
    "\t\tf.write('%s %0.3f %0.2f %s \\n'%(stas[int(Picks_P[i][j,1])], Picks_P[i][j,0] - srcs_trv[i,3], pmap(Picks_P[i][j,-1]), 'P'))\n",
    "\n",
    "\tfor j in range(len(Picks_S[i])):\n",
    "\t\tf.write('%s %0.3f %0.2f %s \\n'%(stas[int(Picks_S[i][j,1])], Picks_S[i][j,0] - srcs_trv[i,3], pmap(Picks_S[i][j,-1]), 'S'))\n",
    "\n",
    "f.close()\n",
    "\n",
    "print('Saved file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fadeef2-16ae-4465-bd1d-d8e53ca114d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "[trv_out2[j,Picks_P[j][:,1].astype('int'),0] - Picks_P[j][:,0] for j in range(len(srcs_trv))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0617a969-2654-4488-87c4-fd091cc1c3ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
